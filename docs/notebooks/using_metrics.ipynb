{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6b1b4f",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05433e5",
   "metadata": {},
   "source": [
    "This notebook provides a practical demonstration of how to use the performance metrics available in the `ThreeWToolkit` to evaluate machine learning models. It covers both classification and regression tasks, illustrating the process from data loading and model training to performance evaluation.\n",
    "\n",
    "### What you will learn:\n",
    "- **Classification Model Evaluation**:\n",
    "    - How to train a classifier on the 3W dataset.\n",
    "    - How to use and interpret the following metrics:\n",
    "        - `accuracy_score`\n",
    "        - `balanced_accuracy_score`\n",
    "        - `average_precision_score`\n",
    "        - `precision_score`\n",
    "        - `recall_score`\n",
    "        - `f1_score`\n",
    "        - `roc_auc_score`\n",
    "\n",
    "- **Regression Model Evaluation**:\n",
    "    - How to set up and train a regression model.\n",
    "    - How to apply and understand the `explained_variance_score`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ff779",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f41cd2",
   "metadata": {},
   "source": [
    "**Adaptation to recognize the project root. For demonstration purposes only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b2266",
   "metadata": {},
   "source": [
    "**Required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0e88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ThreeWToolkit.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    average_precision_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    explained_variance_score,\n",
    ")\n",
    "from ThreeWToolkit.core.base_preprocessing import WindowingConfig\n",
    "from ThreeWToolkit.preprocessing import Windowing\n",
    "from ThreeWToolkit.trainer.trainer import ModelTrainer, TrainerConfig\n",
    "from ThreeWToolkit.models.mlp import MLPConfig\n",
    "from ThreeWToolkit.dataset import ParquetDataset\n",
    "from ThreeWToolkit.core.base_dataset import ParquetDatasetConfig\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b05008",
   "metadata": {},
   "source": [
    "### How to use metrics for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314e430",
   "metadata": {},
   "source": [
    "#### Loading 3W Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bce434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this path to the folder where your dataset is downloaded\n",
    "dataset_path = \"../../dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47373422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ParquetDataset] Dataset found at ../../dataset\n",
      "[ParquetDataset] Validating dataset integrity...\n",
      "[ParquetDataset] Dataset integrity check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'signal':                      ABER-CKGL  ABER-CKP  ESTADO-DHSV  ESTADO-M1  ESTADO-M2  \\\n",
       " timestamp                                                                     \n",
       " 2018-05-13 03:56:05        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 03:56:06        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 03:56:07        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 03:56:08        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 03:56:09        0.0       0.0          0.0        0.0        0.0   \n",
       " ...                        ...       ...          ...        ...        ...   \n",
       " 2018-05-13 11:55:59        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 11:56:00        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 11:56:01        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 11:56:02        0.0       0.0          0.0        0.0        0.0   \n",
       " 2018-05-13 11:56:03        0.0       0.0          0.0        0.0        0.0   \n",
       " \n",
       "                      ESTADO-PXO  ESTADO-SDV-GL  ESTADO-SDV-P  ESTADO-W1  \\\n",
       " timestamp                                                                 \n",
       " 2018-05-13 03:56:05         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 03:56:06         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 03:56:07         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 03:56:08         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 03:56:09         0.0            0.0           0.0        0.0   \n",
       " ...                         ...            ...           ...        ...   \n",
       " 2018-05-13 11:55:59         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 11:56:00         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 11:56:01         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 11:56:02         0.0            0.0           0.0        0.0   \n",
       " 2018-05-13 11:56:03         0.0            0.0           0.0        0.0   \n",
       " \n",
       "                      ESTADO-W2  ...  P-JUS-CKGL  P-JUS-CKP  P-MON-CKP  \\\n",
       " timestamp                       ...                                     \n",
       " 2018-05-13 03:56:05        0.0  ...         0.0        0.0   0.221506   \n",
       " 2018-05-13 03:56:06        0.0  ...         0.0        0.0   0.221506   \n",
       " 2018-05-13 03:56:07        0.0  ...         0.0        0.0   0.221506   \n",
       " 2018-05-13 03:56:08        0.0  ...         0.0        0.0   0.221506   \n",
       " 2018-05-13 03:56:09        0.0  ...         0.0        0.0   0.221506   \n",
       " ...                        ...  ...         ...        ...        ...   \n",
       " 2018-05-13 11:55:59        0.0  ...         0.0        0.0   0.198314   \n",
       " 2018-05-13 11:56:00        0.0  ...         0.0        0.0   0.198314   \n",
       " 2018-05-13 11:56:01        0.0  ...         0.0        0.0   0.198101   \n",
       " 2018-05-13 11:56:02        0.0  ...         0.0        0.0   0.198089   \n",
       " 2018-05-13 11:56:03        0.0  ...         0.0        0.0   0.198088   \n",
       " \n",
       "                         P-PDG     P-TPT  QGL  T-JUS-CKP  T-MON-CKP  T-PDG  \\\n",
       " timestamp                                                                   \n",
       " 2018-05-13 03:56:05  1.214039  1.477511  0.0   1.645573        0.0    0.0   \n",
       " 2018-05-13 03:56:06  1.214029  1.477511  0.0   1.645573        0.0    0.0   \n",
       " 2018-05-13 03:56:07  1.214039  1.477511  0.0   1.645573        0.0    0.0   \n",
       " 2018-05-13 03:56:08  1.214029  1.477511  0.0   1.645573        0.0    0.0   \n",
       " 2018-05-13 03:56:09  1.214039  1.477511  0.0   1.645573        0.0    0.0   \n",
       " ...                       ...       ...  ...        ...        ...    ...   \n",
       " 2018-05-13 11:55:59  2.102652 -1.141288  0.0  -2.040777        0.0    0.0   \n",
       " 2018-05-13 11:56:00  2.102651 -1.141205  0.0  -2.040454        0.0    0.0   \n",
       " 2018-05-13 11:56:01  2.102651 -1.141080  0.0  -2.040174        0.0    0.0   \n",
       " 2018-05-13 11:56:02  2.102652 -1.140957  0.0  -2.039065        0.0    0.0   \n",
       " 2018-05-13 11:56:03  2.102652 -1.140910  0.0  -2.037667        0.0    0.0   \n",
       " \n",
       "                         T-TPT  \n",
       " timestamp                      \n",
       " 2018-05-13 03:56:05  0.966436  \n",
       " 2018-05-13 03:56:06  0.966436  \n",
       " 2018-05-13 03:56:07  0.966436  \n",
       " 2018-05-13 03:56:08  0.966436  \n",
       " 2018-05-13 03:56:09  0.966436  \n",
       " ...                       ...  \n",
       " 2018-05-13 11:55:59 -3.061611  \n",
       " 2018-05-13 11:56:00 -3.061611  \n",
       " 2018-05-13 11:56:01 -3.061612  \n",
       " 2018-05-13 11:56:02 -3.061612  \n",
       " 2018-05-13 11:56:03 -3.061612  \n",
       " \n",
       " [28799 rows x 22 columns],\n",
       " 'label':                      class\n",
       " timestamp                 \n",
       " 2018-05-13 03:56:05      2\n",
       " 2018-05-13 03:56:06      2\n",
       " 2018-05-13 03:56:07      2\n",
       " 2018-05-13 03:56:08      2\n",
       " 2018-05-13 03:56:09      2\n",
       " ...                    ...\n",
       " 2018-05-13 11:55:59      2\n",
       " 2018-05-13 11:56:00      2\n",
       " 2018-05-13 11:56:01      2\n",
       " 2018-05-13 11:56:02      2\n",
       " 2018-05-13 11:56:03      2\n",
       " \n",
       " [28799 rows x 1 columns],\n",
       " 'file_name': PosixPath('2/SIMULATED_00006.parquet')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_config = ParquetDatasetConfig(\n",
    "    path=dataset_path, clean_data=True, target_class=[0, 1, 2]\n",
    ")\n",
    "ds = ParquetDataset(ds_config)\n",
    "ds[19]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204855dc",
   "metadata": {},
   "source": [
    "### Setting up model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766d8f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (activation_func): ReLU()\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "window_size = 1000\n",
    "mlp_config = MLPConfig(\n",
    "    input_size=window_size,\n",
    "    hidden_sizes=(32, 16),\n",
    "    output_size=3,\n",
    "    random_seed=11,\n",
    "    activation_function=\"relu\",\n",
    "    regularization=None,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    optimizer=\"adam\",\n",
    "    criterion=\"cross_entropy\",\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    seed=11,\n",
    "    config_model=mlp_config,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    "    cross_validation=False,\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "trainer = ModelTrainer(trainer_config)\n",
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce99b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking the device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b89185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [00:27<00:00, 27.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Select target columns and prepare training data with windowing\n",
    "selected_col = \"T-TPT\"\n",
    "x_train = []\n",
    "y_train = []\n",
    "dfs = []\n",
    "\n",
    "wind = Windowing(\n",
    "    WindowingConfig(\n",
    "        window=\"hann\", window_size=window_size, overlap=0.5, pad_last_window=True\n",
    "    )\n",
    ")\n",
    "\n",
    "for event in tqdm(ds):\n",
    "    windowed_signal = wind(event[\"signal\"][selected_col])\n",
    "    windowed_signal.drop(columns=[\"win\"], inplace=True)\n",
    "    windowed_signal[\"label\"] = np.unique(event[\"label\"][\"class\"])[0]\n",
    "    dfs.append(windowed_signal)\n",
    "dfs_final = pd.concat(dfs, ignore_index=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1a814",
   "metadata": {},
   "source": [
    "### Split samples into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60065e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (35516, 1000)\n",
      "X_test shape: (8879, 1000)\n",
      "y_train shape: (35516,)\n",
      "y_test shape: (8879,)\n",
      "-------------------------\n",
      "Train occurrences by class:\n",
      "label\n",
      "0    19706\n",
      "1    14614\n",
      "2     1196\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test occurrences by class:\n",
      "label\n",
      "0    4927\n",
      "1    3653\n",
      "2     299\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target (y) from the final dataframe\n",
    "x = dfs_final.iloc[:, :-1]\n",
    "y = dfs_final[\"label\"]\n",
    "\n",
    "# Perform a stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Show the class distribution in the training and testing sets\n",
    "print(\"Train occurrences by class:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest occurrences by class:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "676aabbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb82c291a2f4351bf6604006210394c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Pipeline] Training:   0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the MLP model using the new ModelTrainer interface\n",
    "trainer.train(x_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1f3463",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.Tensor(X_test.to_numpy()).to(device)\n",
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "y_true = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad2f342",
   "metadata": {},
   "source": [
    "### Evaluating the Classification Model\n",
    "\n",
    "With the model trained and predictions made on the test set, we can now evaluate its performance. The following cells demonstrate how to use the various classification metrics imported from `ThreeWToolkit.metrics`. We will use `y_true` (the actual labels) and `y_pred` (the model's predicted labels) to calculate these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e338f",
   "metadata": {},
   "source": [
    "**Accuracy Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6e881",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170a50b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 95.6%.\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "print(f\"The accuracy is {(acc * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab531d5b",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f986bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 95.6%.\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_true))\n",
    "\n",
    "acc = accuracy_score(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n",
    "\n",
    "print(f\"The accuracy is {(acc * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca9f60",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addcc41",
   "metadata": {},
   "source": [
    "**Balanced Accuracy Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecff9990",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f48e0775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy is 87.3%.\n"
     ]
    }
   ],
   "source": [
    "balanced_acc = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "print(f\"The balanced accuracy is {(balanced_acc * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62dc2a",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cff67e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The balanced accuracy is 88.0%.\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_true))\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(\n",
    "    y_true=y_true, y_pred=y_pred, sample_weight=sample_weight\n",
    ")\n",
    "\n",
    "print(f\"The balanced accuracy is {(balanced_acc * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d265",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08841e56",
   "metadata": {},
   "source": [
    "**Average Precision Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3636225",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "266adcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average precision is 95.2%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainer.model(X_test).detach().cpu().numpy()  # needs pseudo-probabilities\n",
    "\n",
    "ap = average_precision_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"The average precision is {(ap * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7fbec1",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0937a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average precision is 88.4%.\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_true))\n",
    "\n",
    "ap = average_precision_score(y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n",
    "\n",
    "print(f\"The average precision is {(ap * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0233077d",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99918f2",
   "metadata": {},
   "source": [
    "**Precision Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaaf295",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "219043ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for class 0 is 93.122%.\n",
      "The precision for class 1 is 99.291%.\n",
      "The precision for class 2 is 97.674%.\n",
      "\n",
      "The weighted average precision is 95.814%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# Calculate precision for each class (one-vs-rest)\n",
    "precision = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "\n",
    "# Print precision for each class\n",
    "for i, p in enumerate(precision):\n",
    "    print(f\"The precision for class {i} is {(p * 100):.3f}%.\")\n",
    "\n",
    "# Calculate weighted average precision\n",
    "precision_weighted = precision_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"\\nThe weighted average precision is {(precision_weighted * 100):.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965855e4",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4cb1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision for class 0 is 92.903%.\n",
      "The precision for class 1 is 99.336%.\n",
      "The precision for class 2 is 98.107%.\n",
      "\n",
      "The weighted average precision is 95.814%.\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_true))\n",
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# Calculate precision for each class (one-vs-rest)\n",
    "precision = precision_score(\n",
    "    y_true=y_true, y_pred=y_pred, average=None, sample_weight=sample_weight\n",
    ")\n",
    "\n",
    "# Print precision for each class\n",
    "for i, p in enumerate(precision):\n",
    "    print(f\"The precision for class {i} is {(p * 100):.3f}%.\")\n",
    "\n",
    "# Calculate weighted average precision\n",
    "precision_weighted = precision_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"\\nThe weighted average precision is {(precision_weighted * 100):.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36134b1",
   "metadata": {},
   "source": [
    "Using different average options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c92b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision [macro] is 96.7%.\n",
      "The precision [micro] is 95.6%.\n",
      "The precision [weighted] is 95.8%.\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_true=y_true, y_pred=y_pred, average=\"macro\")\n",
    "print(f\"The precision [macro] is {(precision * 100):.3}%.\")\n",
    "\n",
    "precision = precision_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "print(f\"The precision [micro] is {(precision * 100):.3}%.\")\n",
    "\n",
    "precision = precision_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"The precision [weighted] is {(precision * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ff910",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8dcf4",
   "metadata": {},
   "source": [
    "**Recall Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bb529",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9afdbeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall for class 0 is 99.756%.\n",
      "The recall for class 1 is 92.034%.\n",
      "The recall for class 2 is 70.234%.\n",
      "\n",
      "The weighted average recall is 95.585%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# Calculate precision for each class (one-vs-rest)\n",
    "recall = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
    "\n",
    "# Print recall for each class\n",
    "for i, p in enumerate(recall):\n",
    "    print(f\"The recall for class {i} is {(p * 100):.3f}%.\")\n",
    "\n",
    "# Calculate weighted average recall\n",
    "recall_weighted = recall_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"\\nThe weighted average recall is {(recall_weighted * 100):.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d1bf3",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ac37cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall for class 0 is 99.729%.\n",
      "The recall for class 1 is 92.170%.\n",
      "The recall for class 2 is 72.208%.\n",
      "\n",
      "The weighted average recall is 95.585%.\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_true))\n",
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# Calculate recall for each class (one-vs-rest)\n",
    "precision = recall_score(\n",
    "    y_true=y_true, y_pred=y_pred, average=None, sample_weight=sample_weight\n",
    ")\n",
    "\n",
    "# Print recall for each class\n",
    "for i, p in enumerate(precision):\n",
    "    print(f\"The recall for class {i} is {(p * 100):.3f}%.\")\n",
    "\n",
    "# Calculate weighted average recall\n",
    "precision_weighted = recall_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"\\nThe weighted average recall is {(precision_weighted * 100):.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f02d5",
   "metadata": {},
   "source": [
    "Using different average options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a001620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall [macro] is 87.3%.\n",
      "The recall [micro] is 95.6%.\n",
      "The recall [weighted] is 95.6%.\n"
     ]
    }
   ],
   "source": [
    "recall = recall_score(y_true=y_true, y_pred=y_pred, average=\"macro\")\n",
    "print(f\"The recall [macro] is {(recall * 100):.3}%.\")\n",
    "\n",
    "recall = recall_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "print(f\"The recall [micro] is {(recall * 100):.3}%.\")\n",
    "\n",
    "recall = recall_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"The recall [weighted] is {(recall * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366437",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ce1e9",
   "metadata": {},
   "source": [
    "**F1 Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30016650",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "927c37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score for class 0 against the rest is 96.402%.\n",
      "The f1 score for class 1 against the rest is 95.579%.\n",
      "The f1 score for class 2 against the rest is 83.118%.\n",
      "\n",
      "The weighted average f1 score is 95.504%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = trainer.model(X_test).detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "# Calculate precision for each class (one-vs-rest)\n",
    "precision = f1_score(\n",
    "    y_true=y_true, y_pred=y_pred, average=None, sample_weight=sample_weight\n",
    ")\n",
    "\n",
    "# Print f1 for each class\n",
    "for i, p in enumerate(precision):\n",
    "    print(f\"The f1 score for class {i} against the rest is {(p * 100):.3f}%.\")\n",
    "\n",
    "# Calculate weighted average f1\n",
    "precision_weighted = f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"\\nThe weighted average f1 score is {(precision_weighted * 100):.3f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadfa65",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f936642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1_score [macro] is 91.2%.\n",
      "The f1_score [micro] is 95.6%.\n",
      "The f1_score [weighted] is 95.5%.\n"
     ]
    }
   ],
   "source": [
    "recall = f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")\n",
    "print(f\"The f1_score [macro] is {(recall * 100):.3}%.\")\n",
    "\n",
    "recall = f1_score(y_true=y_true, y_pred=y_pred, average=\"micro\")\n",
    "print(f\"The f1_score [micro] is {(recall * 100):.3}%.\")\n",
    "\n",
    "recall = f1_score(y_true=y_true, y_pred=y_pred, average=\"weighted\")\n",
    "print(f\"The f1_score [weighted] is {(recall * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8ba889",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429d35d4",
   "metadata": {},
   "source": [
    "**ROC AUC Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5671ffa",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90d74636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for each class (One-vs-Rest):\n",
      "The ROC AUC for class 0 is 96.436%.\n",
      "The ROC AUC for class 1 is 97.318%.\n",
      "The ROC AUC for class 2 is 89.701%.\n",
      "\n",
      "Using multi_class options:\n",
      "The ROC AUC (OVR, macro) is 94.485%.\n",
      "The ROC AUC (OVR, weighted) is 96.572%.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC (OVO, macro) is 93.092%.\n",
      "The roc_auc is 93.6%.\n"
     ]
    }
   ],
   "source": [
    "y_pred = torch.softmax(trainer.model(X_test), dim=1).detach().cpu().numpy()\n",
    "\n",
    "# Calculate ROC AUC for each class (One-vs-Rest)\n",
    "print(\"ROC AUC for each class (One-vs-Rest):\")\n",
    "for i in range(y_pred.shape[1]):\n",
    "    y_true_class = (y_true == i).astype(int)\n",
    "    y_pred_class = y_pred[:, i]\n",
    "    roc_auc = roc_auc_score(y_true=y_true_class, y_pred=y_pred_class)\n",
    "    print(f\"The ROC AUC for class {i} is {(roc_auc * 100):.3f}%.\")\n",
    "\n",
    "print(\"\\nUsing multi_class options:\")\n",
    "# Calculate ROC AUC using One-vs-Rest (ovr) strategy with macro averaging\n",
    "roc_auc_ovr_macro = roc_auc_score(\n",
    "    y_true=y_true, y_pred=y_pred, multi_class=\"ovr\", average=\"macro\"\n",
    ")\n",
    "print(f\"The ROC AUC (OVR, macro) is {(roc_auc_ovr_macro * 100):.3f}%.\")\n",
    "\n",
    "# Calculate ROC AUC using One-vs-Rest (ovr) strategy with weighted averaging\n",
    "roc_auc_ovr_weighted = roc_auc_score(\n",
    "    y_true=y_true, y_pred=y_pred, multi_class=\"ovr\", average=\"weighted\"\n",
    ")\n",
    "print(f\"The ROC AUC (OVR, weighted) is {(roc_auc_ovr_weighted * 100):.3f}%.\")\n",
    "\n",
    "# Calculate ROC AUC using One-vs-One (ovo) strategy with macro averaging\n",
    "roc_auc_ovo_macro = roc_auc_score(\n",
    "    y_true=y_true, y_pred=y_pred, multi_class=\"ovo\", average=\"macro\"\n",
    ")\n",
    "print(f\"The ROC AUC (OVO, macro) is {(roc_auc_ovo_macro * 100):.3f}%.\")\n",
    "\n",
    "# Calculate ROC AUC using One-vs-One (ovo) strategy with weighted averaging\n",
    "roc_auc = roc_auc_score(\n",
    "    y_true=y_true, y_pred=y_pred, multi_class=\"ovo\", average=\"weighted\"\n",
    ")\n",
    "\n",
    "print(f\"The roc_auc is {(roc_auc * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076d124",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b36927e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ParquetDataset] Dataset found at ../../dataset\n",
      "[ParquetDataset] Validating dataset integrity...\n",
      "[ParquetDataset] Dataset integrity check passed!\n",
      ">> ['P-TPT', 'P-PDG', 'T-TPT', 'P-MON-CKP', 'T-JUS-CKP', 'P-JUS-CKGL']\n",
      "X_train shape: (27839, 6)\n",
      "X_test shape: (6960, 6)\n",
      "y_train shape: (27839,)\n",
      "y_test shape: (6960,)\n"
     ]
    }
   ],
   "source": [
    "x_columns = [\"P-TPT\", \"P-PDG\", \"T-TPT\", \"P-MON-CKP\", \"T-JUS-CKP\", \"P-JUS-CKGL\"]\n",
    "\n",
    "ds_config = ParquetDatasetConfig(\n",
    "    columns=x_columns,\n",
    "    target_column=\"QGL\",\n",
    "    path=dataset_path,\n",
    "    clean_data=True,\n",
    ")\n",
    "cleaned_dataset = ParquetDataset(ds_config)\n",
    "\n",
    "# Add some noise to the signal for demonstration purposes\n",
    "file_id = 93\n",
    "x = cleaned_dataset[file_id][\"signal\"] + np.random.normal(\n",
    "    0, 0.01, cleaned_dataset[file_id][\"signal\"].shape\n",
    ")\n",
    "y = cleaned_dataset[file_id][\"label\"][\"QGL\"] + np.random.normal(\n",
    "    0, 1, cleaned_dataset[file_id][\"label\"][\"QGL\"].shape\n",
    ")\n",
    "\n",
    "# Perform a stratified train-test split.\n",
    "# Both X_train and X_test are pd.DataFrames, and y_train and y_test are pd.Series.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94975e6b",
   "metadata": {},
   "source": [
    "### How to use metrics for regression tasks\n",
    "\n",
    "Now, let's explore how to evaluate a model on a regression task. We will set up a new MLP model configured for regression (predicting a continuous value) and then use appropriate metrics to assess its performance.\n",
    "\n",
    "### Setting up and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afae7f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2ee8efa013417383bdc24a34327610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[Pipeline] Training:   0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_config = MLPConfig(\n",
    "    input_size=len(x_columns),\n",
    "    hidden_sizes=(32, 16),\n",
    "    output_size=1,\n",
    "    random_seed=11,\n",
    "    activation_function=\"relu\",\n",
    "    regularization=None,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    optimizer=\"adam\",\n",
    "    criterion=\"mse\",\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    seed=42,\n",
    "    config_model=mlp_config,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    "    cross_validation=False,\n",
    "    shuffle_train=True,\n",
    ")\n",
    "trainer = ModelTrainer(trainer_config)\n",
    "\n",
    "trainer.train(x_train=X_train, y_train=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5da042ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test is initially a pd.DataFrame, comming from the train-test split\n",
    "X_test = torch.Tensor(X_test.to_numpy()).to(device)\n",
    "y_pred = trainer.model(X_test).detach().cpu().numpy()\n",
    "y_true = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36337ba",
   "metadata": {},
   "source": [
    "**Explained Variance Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05b8de",
   "metadata": {},
   "source": [
    "Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "238e3f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explained_variance_score is 90.9%.\n"
     ]
    }
   ],
   "source": [
    "ev_score = explained_variance_score(y_true=y_true, y_pred=y_pred)\n",
    "print(f\"The explained_variance_score is {(ev_score * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e46e94",
   "metadata": {},
   "source": [
    "Using sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91f101ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explained_variance_score is 90.8%.\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.rand(len(y_true))\n",
    "\n",
    "ev_score = explained_variance_score(y_true=y_true, y_pred=y_pred, sample_weight=weights)\n",
    "print(f\"The explained_variance_score is {(ev_score * 100):.3}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567b340",
   "metadata": {},
   "source": [
    "Using different average options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22f3a3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The explained_variance [uniform_average] is 90.86%.\n",
      "The explained_variance [raw_values] is [0.90864036].\n",
      "The explained_variance [variance_weighted] is 90.86%.\n"
     ]
    }
   ],
   "source": [
    "# uniform_average (default)\n",
    "ev_score = explained_variance_score(y_true, y_pred, multioutput=\"uniform_average\")\n",
    "print(f\"The explained_variance [uniform_average] is {(ev_score * 100):.2f}%.\")\n",
    "\n",
    "# raw_values: retorna um valor por saída\n",
    "ev_score = explained_variance_score(y_true, y_pred, multioutput=\"raw_values\")\n",
    "print(f\"The explained_variance [raw_values] is {ev_score}.\")\n",
    "\n",
    "# variance_weighted\n",
    "ev_score = explained_variance_score(y_true, y_pred, multioutput=\"variance_weighted\")\n",
    "print(f\"The explained_variance [variance_weighted] is {(ev_score * 100):.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca196c27",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bab0cac",
   "metadata": {},
   "source": [
    "# 7. ðŸ¤– Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3632d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook demonstrates how to train and evaluate machine learning models using the 3WToolkit. We will cover a complete workflow from data preparation to model assessment, including:\n",
    "\n",
    "- Loading and preprocessing the 3W dataset\n",
    "- Training MLP models with different configurations\n",
    "- Evaluating model performance using various metrics\n",
    "- Visualizing training history and results\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Configure and train MLP models using the 3WToolkit\n",
    "- Evaluate model performance using comprehensive metrics\n",
    "- Visualize training progress and model predictions\n",
    "- Understand the complete model training workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3c221",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437797a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from ThreeWToolkit.preprocessing import Windowing\n",
    "from ThreeWToolkit.core.base_preprocessing import WindowingConfig\n",
    "from ThreeWToolkit.trainer.trainer import ModelTrainer, TrainerConfig\n",
    "from ThreeWToolkit.models.mlp import MLPConfig\n",
    "from ThreeWToolkit.dataset import ParquetDataset\n",
    "from ThreeWToolkit.core.base_dataset import ParquetDatasetConfig\n",
    "from ThreeWToolkit.core.base_assessment import ModelAssessmentConfig\n",
    "from ThreeWToolkit.assessment.model_assess import ModelAssessment\n",
    "from ThreeWToolkit.core.enums import TaskType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624e924",
   "metadata": {},
   "source": [
    "Let's create a ParquetDataset that loads cleaned data with target classes 0, 1, and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b343f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset path\n",
    "dataset_path = Path(\"../../dataset\")\n",
    "\n",
    "# Create and load dataset with target classes 0, 1, and 2\n",
    "ds_config = ParquetDatasetConfig(\n",
    "    path=dataset_path, \n",
    "    clean_data=True, \n",
    "    download=False, \n",
    "    target_class=[0, 1, 2]\n",
    ")\n",
    "ds = ParquetDataset(ds_config)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Total events: {len(ds)}\")\n",
    "print(f\"Sample event structure: {ds[0].keys()}\")\n",
    "print(f\"Sample label: {ds[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa8a48",
   "metadata": {},
   "source": [
    "## 2. Model Configuration and Trainer Setup\n",
    "\n",
    "**II. Instantiating configuration classes for the MLP model, Training parameters, and Evaluation parameters.**\n",
    "\n",
    "With the ParquetDataset instance defined, we can set the parameters for the MLP model using the MLPConfig object. These parameters will be combined with the TrainerConfig and managed through the ModelTrainer, which encapsulates the training workflow.\n",
    "\n",
    "The defined workflow controls most relevant parameters that will be used for training of a model. \n",
    "\n",
    "Finally, the ModelTrainer is instantiated with the training configuration, while the ModelAssessment object prepares the evaluation pipeline. The model architecture can be visualized by printing the `trainer.model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window size for the model\n",
    "window_size = 100\n",
    "\n",
    "# Configure the MLP model\n",
    "mlp_config = MLPConfig(\n",
    "    input_size=window_size,\n",
    "    hidden_sizes=(32, 16),\n",
    "    output_size=3,  # 3 classes: 0, 1, 2\n",
    "    random_seed=11,\n",
    "    activation_function=\"relu\",\n",
    "    regularization=None,\n",
    ")\n",
    "\n",
    "# Configure the trainer\n",
    "trainer_config = TrainerConfig(\n",
    "    optimizer=\"adam\",\n",
    "    criterion=\"cross_entropy\",\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    seed=11,\n",
    "    config_model=mlp_config,\n",
    "    learning_rate=0.001,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    cross_validation=False,\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "# Configure model assessment\n",
    "assessment_config = ModelAssessmentConfig(\n",
    "    metrics=[\"balanced_accuracy\", \"precision\", \"recall\", \"f1\"],\n",
    "    task_type=TaskType.CLASSIFICATION,\n",
    "    class_names=[\"Class_0\", \"Class_1\", \"Class_2\"],\n",
    "    export_results=True,\n",
    "    generate_report=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer and assessor\n",
    "trainer = ModelTrainer(trainer_config)\n",
    "assessor = ModelAssessment(assessment_config)\n",
    "\n",
    "# Display the model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(trainer.model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56135cc",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing with Windowing\n",
    "\n",
    "**III. Preprocessing the data**\n",
    "\n",
    "The next step is to iterate over a dataset of time series events, applying a windowing function to a selected signal column, in this case \"T-TPT\".\n",
    "\n",
    "All windowed segments from all events are then concatenated into a single DataFrame (dfs_final). This prepares the data for supervised training, where each row represents a windowed segment with its corresponding class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target columns and prepare training data with windowing\n",
    "selected_col = \"T-TPT\"\n",
    "dfs = []\n",
    "\n",
    "# Configure windowing\n",
    "wind = Windowing(WindowingConfig(\n",
    "    window=\"hann\",\n",
    "    window_size=window_size,\n",
    "    overlap=0.5,\n",
    "    pad_last_window=True\n",
    "))\n",
    "\n",
    "print(\"Processing events with windowing...\")\n",
    "for event in tqdm(ds):\n",
    "    # Apply windowing to the selected column\n",
    "    windowed_signal = wind(event[\"signal\"][selected_col])\n",
    "    \n",
    "    # Remove the window column (not needed for training)\n",
    "    windowed_signal.drop(columns=[\"win\"], inplace=True)\n",
    "    \n",
    "    # Add the label for this event\n",
    "    windowed_signal[\"label\"] = np.unique(event[\"label\"][\"class\"])[0]\n",
    "    \n",
    "    dfs.append(windowed_signal)\n",
    "\n",
    "# Concatenate all windowed data\n",
    "dfs_final = pd.concat(dfs, ignore_index=True, axis=0)\n",
    "\n",
    "print(f\"\\nPreprocessing completed!\")\n",
    "print(f\"Total windows: {len(dfs_final)}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Features per window: {dfs_final.shape[1] - 1}\")  # -1 for label column\n",
    "print(f\"Label distribution:\")\n",
    "print(dfs_final[\"label\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6e1fe",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "**IV. Training**\n",
    "\n",
    "Finally we can call the train function using the trainer object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "x_train = dfs_final.iloc[:, :-1]  # All columns except the last one (label)\n",
    "y_train = dfs_final[\"label\"].astype(int)  # Convert labels to integers\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "# Train the MLP model using the ModelTrainer interface\n",
    "trainer.train(x_train=x_train, y_train=y_train)\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3879ae1",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Assessment\n",
    "\n",
    "**V. Assessment**\n",
    "\n",
    "The trainer class allows direct evaluation of the trained model using the `assess` method, which returns a dictionary containing performance metrics and evaluation parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct assessment using trainer\n",
    "print(\"=== Method 1: Direct Assessment using Trainer ===\")\n",
    "trainer_results = trainer.assess(x_train, y_train, assessment_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a5cc3",
   "metadata": {},
   "source": [
    "**Another option, and the most recommended one, is to use the `ModelAssessment` class to perform the evaluation of the results.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using ModelAssessment class (recommended)\n",
    "print(\"\\n=== Method 2: Using ModelAssessment Class (Recommended) ===\")\n",
    "results = assessor.evaluate(trainer.model, x_train, y_train)\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1fdc9",
   "metadata": {},
   "source": [
    "### Retrieving Aggregated Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary metrics\n",
    "print(\"=== Model Performance Summary ===\")\n",
    "print(assessor.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e773518",
   "metadata": {},
   "source": [
    "## 6. Training History Visualization\n",
    "\n",
    "The trainer object also collects a history of validation and training loss that can be visualized after the training is completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcea378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trainer.history[0][\"val_loss\"], label=\"Validation Loss\", marker='o')\n",
    "plt.plot(trainer.history[0][\"train_loss\"], label=\"Training Loss\", marker='s')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training and validation accuracy (if available)\n",
    "plt.subplot(1, 2, 2)\n",
    "if \"val_accuracy\" in trainer.history[0] and \"train_accuracy\" in trainer.history[0]:\n",
    "    plt.plot(trainer.history[0][\"val_accuracy\"], label=\"Validation Accuracy\", marker='o')\n",
    "    plt.plot(trainer.history[0][\"train_accuracy\"], label=\"Training Accuracy\", marker='s')\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, \"Accuracy metrics not available\\nin this training configuration\", \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title(\"Training Metrics\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final training statistics\n",
    "print(\"=== Final Training Statistics ===\")\n",
    "print(f\"Final Training Loss: {trainer.history[0]['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {trainer.history[0]['val_loss'][-1]:.4f}\")\n",
    "if \"val_accuracy\" in trainer.history[0]:\n",
    "    print(f\"Final Training Accuracy: {trainer.history[0]['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {trainer.history[0]['val_accuracy'][-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777d1cb",
   "metadata": {},
   "source": [
    "## 7. Additional Analysis and Insights\n",
    "\n",
    "Let's perform some additional analysis to better understand our model's performance and the data characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e57425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data characteristics\n",
    "print(\"=== Data Analysis ===\")\n",
    "print(f\"Dataset size: {len(ds)} events\")\n",
    "print(f\"Total windows after preprocessing: {len(dfs_final)}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Overlap: 50%\")\n",
    "\n",
    "# Class distribution analysis\n",
    "print(f\"\\nClass Distribution:\")\n",
    "class_counts = dfs_final[\"label\"].value_counts().sort_index()\n",
    "for class_id, count in class_counts.items():\n",
    "    percentage = (count / len(dfs_final)) * 100\n",
    "    print(f\"  Class {class_id}: {count} windows ({percentage:.1f}%)\")\n",
    "\n",
    "# Model complexity analysis\n",
    "print(f\"\\nModel Complexity:\")\n",
    "print(f\"  Input features: {window_size}\")\n",
    "print(f\"  Hidden layers: {len(mlp_config.hidden_sizes)}\")\n",
    "print(f\"  Hidden units: {mlp_config.hidden_sizes}\")\n",
    "print(f\"  Output classes: {mlp_config.output_size}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in trainer.model.parameters())}\")\n",
    "\n",
    "# Training configuration summary\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Optimizer: {trainer_config.optimizer}\")\n",
    "print(f\"  Learning rate: {trainer_config.learning_rate}\")\n",
    "print(f\"  Batch size: {trainer_config.batch_size}\")\n",
    "print(f\"  Epochs: {trainer_config.epochs}\")\n",
    "print(f\"  Device: {trainer_config.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4862a6a",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "In this notebook, we've successfully demonstrated the complete model training and evaluation workflow using the 3WToolkit v2.0.0:\n",
    "\n",
    "1. **Dataset Loading**: Loaded the 3W dataset with cleaned data for classes 0, 1, and 2\n",
    "2. **Model Configuration**: Set up an MLP model with appropriate architecture for our classification task\n",
    "3. **Data Preprocessing**: Applied windowing to convert time series into fixed-size windows suitable for training\n",
    "4. **Model Training**: Trained the MLP model using the configured trainer\n",
    "5. **Model Evaluation**: Assessed model performance using comprehensive metrics\n",
    "6. **Visualization**: Created plots to understand training progress and model behavior\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- The windowing approach allows us to treat time series segments as independent samples\n",
    "- The MLP model can effectively learn patterns from windowed time series data\n",
    "- The 3WToolkit provides comprehensive evaluation metrics and visualization tools\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further improve your model, consider:\n",
    "\n",
    "1. **Experiment with different window sizes** to find the optimal balance between context and computational efficiency\n",
    "2. **Try different model architectures** (deeper networks, different activation functions)\n",
    "3. **Implement cross-validation** for more robust performance estimation\n",
    "4. **Use feature extraction methods** (statistical, wavelet, or exponentially weighted features) as shown in the previous notebooks\n",
    "5. **Experiment with different preprocessing strategies** (normalization, imputation methods)\n",
    "6. **Try ensemble methods** or other advanced techniques\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- Check out the Pipeline Integration notebook for automated workflows\n",
    "- Explore the Feature Extraction notebooks for advanced preprocessing techniques\n",
    "- Review the Data Visualization notebooks for comprehensive analysis tools\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

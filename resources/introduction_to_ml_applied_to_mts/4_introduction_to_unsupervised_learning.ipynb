{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be897dda",
   "metadata": {},
   "source": [
    "# üìö Unsupervised Learning and Novelty Detection (5 minutes)\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "**Unsupervised Learning** is a machine learning approach where we analyze data without labeled examples, seeking to discover hidden patterns, structures, or anomalies in the data.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Unlabeled Data**: Input data without target labels\n",
    "2. **Pattern Discovery**: Finding hidden structures in data\n",
    "3. **Anomaly Detection**: Identifying unusual or outlier patterns\n",
    "4. **Dimensionality Reduction**: Learning compact representations\n",
    "5. **Reconstruction**: Learning to recreate input data\n",
    "\n",
    "### Our Novelty Detection Problem: 3W Oil Well Anomaly Detection\n",
    "\n",
    "- **Objective**: Detect abnormal oil well operational states using only normal operation data\n",
    "- **Training Data**: Only class 0 (normal operation) sensor measurements\n",
    "- **Detection Target**: Identify when sensor patterns deviate from normal behavior\n",
    "- **Challenge**: Learn normal patterns to detect any deviation as potential fault\n",
    "\n",
    "### Why Novelty Detection Matters in Oil Wells:\n",
    "- **Early Warning System**: Detect problems before they escalate\n",
    "- **Unsupervised Monitoring**: No need for labeled fault examples\n",
    "- **Operational Safety**: Continuous monitoring of normal vs abnormal states\n",
    "- **Preventive Maintenance**: Identify subtle deviations before major failures\n",
    "\n",
    "### Autoencoder-Based Novelty Detection Approach:\n",
    "- **Training Phase**: Learn to reconstruct only normal operation patterns (class 0)\n",
    "- **Detection Phase**: High reconstruction error indicates potential anomaly\n",
    "- **Threshold**: Statistical approach (mean + 3√óstd) for anomaly scoring\n",
    "- **Evaluation**: Test how well it distinguishes normal vs fault classes\n",
    "\n",
    "### Problem Characteristics:\n",
    "- **Normal-Only Training**: Learn patterns from class 0 data exclusively\n",
    "- **Time Series**: Sequential sensor measurements requiring LSTM architecture\n",
    "- **High Dimensional**: Many sensors √ó time steps = complex patterns\n",
    "- **Reconstruction-Based**: Anomalies have higher reconstruction errors\n",
    "- **Threshold-Based**: Statistical approach for anomaly detection\n",
    "\n",
    "Let's explore LSTM autoencoders for oil well novelty detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce29f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 3W Dataset for Autoencoder Novelty Detection\n",
      "============================================================\n",
      "Importing modules... ‚úÖ\n",
      "Anomaly Class Configuration: [3, 4, 8]\n",
      "Optimization Settings:\n",
      "   ‚Ä¢ Single fold loading: True\n",
      "   ‚Ä¢ Target fold: fold_1\n",
      "   ‚Ä¢ Sampling enabled: True\n",
      "   ‚Ä¢ Max normal samples: 2000\n",
      "   ‚Ä¢ Max anomaly samples: 1000\n",
      "   ‚Ä¢ Selected anomaly classes: [3, 4, 8]\n",
      "\n",
      "Initializing data persistence... ‚úÖ\n",
      "Using format: pickle\n",
      "Checking windowed directory: processed_data\\cv_splits\\windowed... ‚úÖ\n",
      "Looking for fold directories... ‚úÖ Found 3 folds: ['fold_1', 'fold_2', 'fold_3']\n",
      "Using single fold: fold_1\n",
      "\n",
      "Processing fold_1 (1/1)...\n",
      "   Loading train data... ‚úÖ\n",
      "Anomaly Class Configuration: [3, 4, 8]\n",
      "Optimization Settings:\n",
      "   ‚Ä¢ Single fold loading: True\n",
      "   ‚Ä¢ Target fold: fold_1\n",
      "   ‚Ä¢ Sampling enabled: True\n",
      "   ‚Ä¢ Max normal samples: 2000\n",
      "   ‚Ä¢ Max anomaly samples: 1000\n",
      "   ‚Ä¢ Selected anomaly classes: [3, 4, 8]\n",
      "\n",
      "Initializing data persistence... ‚úÖ\n",
      "Using format: pickle\n",
      "Checking windowed directory: processed_data\\cv_splits\\windowed... ‚úÖ\n",
      "Looking for fold directories... ‚úÖ Found 3 folds: ['fold_1', 'fold_2', 'fold_3']\n",
      "Using single fold: fold_1\n",
      "\n",
      "Processing fold_1 (1/1)...\n",
      "   Loading train data... ‚úÖ (168051 windows)\n",
      "      Current totals: 2000 normal, 1000 anomaly\n",
      "      Sampling limits reached - stopping early\n",
      "   Sampling complete - sufficient data collected\n",
      "\n",
      "‚úÖ Successfully loaded and separated windowed data!\n",
      "Normal operation windows (class 0): 2000\n",
      "Anomaly windows (classes [3, 4, 8]): 1000\n",
      "Loading time: 11.140 seconds\n",
      "Files processed: 1\n",
      "\n",
      "Processing sample windows... ‚úÖ\n",
      "\n",
      "Sample Normal Window (Window #1):\n",
      "   ‚Ä¢ Shape: (300, 4)\n",
      "   ‚Ä¢ Class: 0 (Normal Operation)\n",
      "   ‚Ä¢ Features: ['P-PDG_scaled', 'P-TPT_scaled', 'T-TPT_scaled', 'class']\n",
      "\n",
      "Normal Operation Data (Class 0):\n",
      "   ‚Ä¢ Total windows: 2000\n",
      "   ‚Ä¢ Usage: Autoencoder training\n",
      "\n",
      "Anomaly Data (Selected Classes [3, 4, 8]):\n",
      "   ‚Ä¢ Class 3: 1000 windows\n",
      "   ‚Ä¢ Total anomaly windows: 1000\n",
      "   ‚Ä¢ Usage: Anomaly detection testing\n",
      "\n",
      "Performance Summary:\n",
      "   ‚Ä¢ Total execution time: 14.475 seconds\n",
      "   ‚Ä¢ Data loading time: 11.140 seconds\n",
      "   ‚Ä¢ File format: pickle\n",
      "   ‚Ä¢ Folds processed: 1\n",
      "\n",
      "Dataset Summary for Novelty Detection:\n",
      "   ‚Ä¢ Normal training data: 2000 windows\n",
      "   ‚Ä¢ Anomaly test data: 1000 windows\n",
      "   ‚Ä¢ Window dimensions: (300, 4)\n",
      "   ‚Ä¢ Selected anomaly classes: [np.str_('3')]\n",
      "\n",
      "Configuration Notes:\n",
      "   ‚Ä¢ Sampling enabled for faster processing\n",
      "   ‚Ä¢ Only classes [3, 4, 8] used as anomalies\n",
      "   ‚Ä¢ To use full dataset: Set ENABLE_SAMPLING = False\n",
      "   ‚Ä¢ To use all folds: Set USE_SINGLE_FOLD = False\n",
      "‚úÖ (168051 windows)\n",
      "      Current totals: 2000 normal, 1000 anomaly\n",
      "      Sampling limits reached - stopping early\n",
      "   Sampling complete - sufficient data collected\n",
      "\n",
      "‚úÖ Successfully loaded and separated windowed data!\n",
      "Normal operation windows (class 0): 2000\n",
      "Anomaly windows (classes [3, 4, 8]): 1000\n",
      "Loading time: 11.140 seconds\n",
      "Files processed: 1\n",
      "\n",
      "Processing sample windows... ‚úÖ\n",
      "\n",
      "Sample Normal Window (Window #1):\n",
      "   ‚Ä¢ Shape: (300, 4)\n",
      "   ‚Ä¢ Class: 0 (Normal Operation)\n",
      "   ‚Ä¢ Features: ['P-PDG_scaled', 'P-TPT_scaled', 'T-TPT_scaled', 'class']\n",
      "\n",
      "Normal Operation Data (Class 0):\n",
      "   ‚Ä¢ Total windows: 2000\n",
      "   ‚Ä¢ Usage: Autoencoder training\n",
      "\n",
      "Anomaly Data (Selected Classes [3, 4, 8]):\n",
      "   ‚Ä¢ Class 3: 1000 windows\n",
      "   ‚Ä¢ Total anomaly windows: 1000\n",
      "   ‚Ä¢ Usage: Anomaly detection testing\n",
      "\n",
      "Performance Summary:\n",
      "   ‚Ä¢ Total execution time: 14.475 seconds\n",
      "   ‚Ä¢ Data loading time: 11.140 seconds\n",
      "   ‚Ä¢ File format: pickle\n",
      "   ‚Ä¢ Folds processed: 1\n",
      "\n",
      "Dataset Summary for Novelty Detection:\n",
      "   ‚Ä¢ Normal training data: 2000 windows\n",
      "   ‚Ä¢ Anomaly test data: 1000 windows\n",
      "   ‚Ä¢ Window dimensions: (300, 4)\n",
      "   ‚Ä¢ Selected anomaly classes: [np.str_('3')]\n",
      "\n",
      "Configuration Notes:\n",
      "   ‚Ä¢ Sampling enabled for faster processing\n",
      "   ‚Ä¢ Only classes [3, 4, 8] used as anomalies\n",
      "   ‚Ä¢ To use full dataset: Set ENABLE_SAMPLING = False\n",
      "   ‚Ä¢ To use all folds: Set USE_SINGLE_FOLD = False\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# OPTIMIZED LOADING: 3W DATASET FOR UNSUPERVISED NOVELTY DETECTION\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Loading 3W Dataset for Autoencoder Novelty Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import data loading utilities\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "print(\"Importing modules...\", end=\" \")\n",
    "from src.data_persistence import DataPersistence\n",
    "from src import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ\")\n",
    "\n",
    "# ============================================================\n",
    "# OPTIMIZATION SETTINGS - ADJUST FOR SPEED vs COMPLETENESS\n",
    "# ============================================================\n",
    "USE_SINGLE_FOLD = (\n",
    "    True  # True: Fast loading (one fold), False: Complete dataset (all folds)\n",
    ")\n",
    "TARGET_FOLD = \"fold_1\"  # Which fold to use for single fold loading\n",
    "MAX_NORMAL_SAMPLES = 2000  # Limit normal samples for faster processing\n",
    "MAX_ANOMALY_SAMPLES = 1000  # Limit anomaly samples for faster processing\n",
    "ENABLE_SAMPLING = True  # True: Apply sampling limits, False: Load all available data\n",
    "\n",
    "# ============================================================\n",
    "# ANOMALY CLASS CONFIGURATION: ONLY CLASSES 3, 4, 8\n",
    "# ============================================================\n",
    "SELECTED_ANOMALY_CLASSES = [3, 4, 8]  # Only use these classes as anomalies\n",
    "print(f\"Anomaly Class Configuration: {SELECTED_ANOMALY_CLASSES}\")\n",
    "\n",
    "print(f\"Optimization Settings:\")\n",
    "print(f\"   ‚Ä¢ Single fold loading: {USE_SINGLE_FOLD}\")\n",
    "if USE_SINGLE_FOLD:\n",
    "    print(f\"   ‚Ä¢ Target fold: {TARGET_FOLD}\")\n",
    "print(f\"   ‚Ä¢ Sampling enabled: {ENABLE_SAMPLING}\")\n",
    "if ENABLE_SAMPLING:\n",
    "    print(f\"   ‚Ä¢ Max normal samples: {MAX_NORMAL_SAMPLES}\")\n",
    "    print(f\"   ‚Ä¢ Max anomaly samples: {MAX_ANOMALY_SAMPLES}\")\n",
    "print(f\"   ‚Ä¢ Selected anomaly classes: {SELECTED_ANOMALY_CLASSES}\")\n",
    "\n",
    "try:\n",
    "    print(f\"\\nInitializing data persistence...\", end=\" \")\n",
    "    persistence = DataPersistence(base_dir=config.PROCESSED_DATA_DIR, verbose=False)\n",
    "    print(\"‚úÖ\")\n",
    "\n",
    "    print(f\"Using format: {config.SAVE_FORMAT}\")\n",
    "\n",
    "    # Check if windowed directory exists\n",
    "    windowed_dir = os.path.join(persistence.cv_splits_dir, \"windowed\")\n",
    "    print(f\"Checking windowed directory: {windowed_dir}...\", end=\" \")\n",
    "\n",
    "    if not os.path.exists(windowed_dir):\n",
    "        print(\"‚ùå\")\n",
    "        print(\n",
    "            \"‚ùå No windowed data directory found. Please run Data Treatment notebook first to generate windowed time series data.\"\n",
    "        )\n",
    "        raise FileNotFoundError(\"Windowed data directory not found\")\n",
    "    else:\n",
    "        print(\"‚úÖ\")\n",
    "\n",
    "        # Look for fold directories\n",
    "        print(\"Looking for fold directories...\", end=\" \")\n",
    "        fold_dirs = [\n",
    "            d\n",
    "            for d in os.listdir(windowed_dir)\n",
    "            if d.startswith(\"fold_\") and os.path.isdir(os.path.join(windowed_dir, d))\n",
    "        ]\n",
    "        fold_dirs.sort()\n",
    "        print(f\"‚úÖ Found {len(fold_dirs)} folds: {fold_dirs}\")\n",
    "\n",
    "        if not fold_dirs:\n",
    "            print(\"‚ùå No fold directories found in windowed data.\")\n",
    "            raise FileNotFoundError(\"No fold directories found\")\n",
    "\n",
    "        # Determine which folds to process\n",
    "        if USE_SINGLE_FOLD:\n",
    "            if TARGET_FOLD in fold_dirs:\n",
    "                process_folds = [TARGET_FOLD]\n",
    "                print(f\"Using single fold: {TARGET_FOLD}\")\n",
    "            else:\n",
    "                process_folds = [fold_dirs[0]]  # Use first available fold\n",
    "                print(\n",
    "                    f\"‚ö†Ô∏è Target fold '{TARGET_FOLD}' not found, using: {process_folds[0]}\"\n",
    "                )\n",
    "        else:\n",
    "            process_folds = fold_dirs\n",
    "            print(f\"Using all {len(fold_dirs)} folds\")\n",
    "\n",
    "        # Separate data containers for normal (class 0) and anomaly (selected classes only) data\n",
    "        normal_windows = []  # Class 0 for autoencoder training\n",
    "        normal_classes = []\n",
    "        anomaly_windows = []  # Only selected classes for anomaly testing\n",
    "        anomaly_classes = []\n",
    "\n",
    "        load_start = time.time()\n",
    "        total_files_processed = 0\n",
    "\n",
    "        for fold_idx, fold_name in enumerate(process_folds):\n",
    "            fold_path = os.path.join(windowed_dir, fold_name)\n",
    "\n",
    "            print(\n",
    "                f\"\\nProcessing {fold_name} ({fold_idx + 1}/{len(process_folds)})...\"\n",
    "            )\n",
    "\n",
    "            # Process training and test data\n",
    "            for data_type in [\"train\", \"test\"]:\n",
    "                print(f\"   Loading {data_type} data...\", end=\" \")\n",
    "\n",
    "                # Try pickle first, then parquet\n",
    "                pickle_file = os.path.join(\n",
    "                    fold_path, f\"{data_type}_windowed.{config.SAVE_FORMAT}\"\n",
    "                )\n",
    "                parquet_file = os.path.join(fold_path, f\"{data_type}_windowed.parquet\")\n",
    "\n",
    "                fold_dfs, fold_classes = [], []\n",
    "\n",
    "                if os.path.exists(pickle_file):\n",
    "                    try:\n",
    "                        fold_dfs, fold_classes = persistence._load_dataframes(\n",
    "                            pickle_file, config.SAVE_FORMAT\n",
    "                        )\n",
    "                        print(f\"‚úÖ ({len(fold_dfs)} windows)\")\n",
    "                        total_files_processed += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Pickle error: {str(e)}\")\n",
    "                        # Try parquet as fallback\n",
    "                        if os.path.exists(parquet_file):\n",
    "                            try:\n",
    "                                fold_dfs, fold_classes = persistence._load_from_parquet(\n",
    "                                    parquet_file\n",
    "                                )\n",
    "                                print(f\"‚úÖ Parquet fallback ({len(fold_dfs)} windows)\")\n",
    "                                total_files_processed += 1\n",
    "                            except Exception as e2:\n",
    "                                print(f\"‚ùå Parquet fallback failed: {str(e2)}\")\n",
    "\n",
    "                elif os.path.exists(parquet_file):\n",
    "                    try:\n",
    "                        fold_dfs, fold_classes = persistence._load_from_parquet(\n",
    "                            parquet_file\n",
    "                        )\n",
    "                        print(f\"‚úÖ ({len(fold_dfs)} windows)\")\n",
    "                        total_files_processed += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Parquet error: {str(e)}\")\n",
    "                else:\n",
    "                    print(\"‚ùå No data file found\")\n",
    "\n",
    "                # Separate normal (class 0) from selected anomaly classes only\n",
    "                for df, cls in zip(fold_dfs, fold_classes):\n",
    "                    if str(cls) == \"0\":  # Normal operation\n",
    "                        if (\n",
    "                            not ENABLE_SAMPLING\n",
    "                            or len(normal_windows) < MAX_NORMAL_SAMPLES\n",
    "                        ):\n",
    "                            normal_windows.append(df)\n",
    "                            normal_classes.append(cls)\n",
    "                    elif int(cls) in SELECTED_ANOMALY_CLASSES:  # Only selected fault classes\n",
    "                        if (\n",
    "                            not ENABLE_SAMPLING\n",
    "                            or len(anomaly_windows) < MAX_ANOMALY_SAMPLES\n",
    "                        ):\n",
    "                            anomaly_windows.append(df)\n",
    "                            anomaly_classes.append(cls)\n",
    "\n",
    "                # Show progress\n",
    "                if ENABLE_SAMPLING:\n",
    "                    print(\n",
    "                        f\"      Current totals: {len(normal_windows)} normal, {len(anomaly_windows)} anomaly\"\n",
    "                    )\n",
    "                    if (\n",
    "                        len(normal_windows) >= MAX_NORMAL_SAMPLES\n",
    "                        and len(anomaly_windows) >= MAX_ANOMALY_SAMPLES\n",
    "                    ):\n",
    "                        print(f\"      Sampling limits reached - stopping early\")\n",
    "                        break\n",
    "\n",
    "            # Early exit if sampling limits reached\n",
    "            if (\n",
    "                ENABLE_SAMPLING\n",
    "                and len(normal_windows) >= MAX_NORMAL_SAMPLES\n",
    "                and len(anomaly_windows) >= MAX_ANOMALY_SAMPLES\n",
    "            ):\n",
    "                print(f\"   Sampling complete - sufficient data collected\")\n",
    "                break\n",
    "\n",
    "        load_time = time.time() - load_start\n",
    "\n",
    "        if normal_windows and anomaly_windows:\n",
    "            print(f\"\\n‚úÖ Successfully loaded and separated windowed data!\")\n",
    "            print(f\"Normal operation windows (class 0): {len(normal_windows)}\")\n",
    "            print(f\"Anomaly windows (classes {SELECTED_ANOMALY_CLASSES}): {len(anomaly_windows)}\")\n",
    "            print(f\"Loading time: {load_time:.3f} seconds\")\n",
    "            print(f\"Files processed: {total_files_processed}\")\n",
    "\n",
    "            # Display sample window information\n",
    "            if normal_windows:\n",
    "                print(\"\\nProcessing sample windows...\", end=\" \")\n",
    "                first_normal_window = normal_windows[0]\n",
    "                print(\"‚úÖ\")\n",
    "\n",
    "                print(f\"\\nSample Normal Window (Window #1):\")\n",
    "                print(f\"   ‚Ä¢ Shape: {first_normal_window.shape}\")\n",
    "                print(f\"   ‚Ä¢ Class: {normal_classes[0]} (Normal Operation)\")\n",
    "                print(f\"   ‚Ä¢ Features: {list(first_normal_window.columns)}\")\n",
    "\n",
    "                # Show class distribution\n",
    "                print(f\"\\nNormal Operation Data (Class 0):\")\n",
    "                print(f\"   ‚Ä¢ Total windows: {len(normal_windows)}\")\n",
    "                print(f\"   ‚Ä¢ Usage: Autoencoder training\")\n",
    "\n",
    "                print(f\"\\nAnomaly Data (Selected Classes {SELECTED_ANOMALY_CLASSES}):\")\n",
    "                anomaly_unique, anomaly_counts = np.unique(\n",
    "                    anomaly_classes, return_counts=True\n",
    "                )\n",
    "                for cls, count in zip(anomaly_unique, anomaly_counts):\n",
    "                    print(f\"   ‚Ä¢ Class {cls}: {count} windows\")\n",
    "                print(f\"   ‚Ä¢ Total anomaly windows: {len(anomaly_windows)}\")\n",
    "                print(f\"   ‚Ä¢ Usage: Anomaly detection testing\")\n",
    "\n",
    "                total_time = time.time() - start_time\n",
    "                print(f\"\\nPerformance Summary:\")\n",
    "                print(f\"   ‚Ä¢ Total execution time: {total_time:.3f} seconds\")\n",
    "                print(f\"   ‚Ä¢ Data loading time: {load_time:.3f} seconds\")\n",
    "                print(f\"   ‚Ä¢ File format: {config.SAVE_FORMAT}\")\n",
    "                print(f\"   ‚Ä¢ Folds processed: {len(process_folds)}\")\n",
    "\n",
    "                print(f\"\\nDataset Summary for Novelty Detection:\")\n",
    "                print(f\"   ‚Ä¢ Normal training data: {len(normal_windows)} windows\")\n",
    "                print(f\"   ‚Ä¢ Anomaly test data: {len(anomaly_windows)} windows\")\n",
    "                print(f\"   ‚Ä¢ Window dimensions: {first_normal_window.shape}\")\n",
    "                print(f\"   ‚Ä¢ Selected anomaly classes: {sorted(anomaly_unique)}\")\n",
    "\n",
    "                if ENABLE_SAMPLING:\n",
    "                    print(f\"\\nConfiguration Notes:\")\n",
    "                    print(f\"   ‚Ä¢ Sampling enabled for faster processing\")\n",
    "                    print(f\"   ‚Ä¢ Only classes {SELECTED_ANOMALY_CLASSES} used as anomalies\")\n",
    "                    print(f\"   ‚Ä¢ To use full dataset: Set ENABLE_SAMPLING = False\")\n",
    "                    print(f\"   ‚Ä¢ To use all folds: Set USE_SINGLE_FOLD = False\")\n",
    "\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No normal operation windows found\")\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Insufficient data found for novelty detection\")\n",
    "            print(f\"   ‚Ä¢ Normal windows: {len(normal_windows)}\")\n",
    "            print(f\"   ‚Ä¢ Anomaly windows: {len(anomaly_windows)}\")\n",
    "            normal_windows = []\n",
    "            normal_classes = []\n",
    "            anomaly_windows = []\n",
    "            anomaly_classes = []\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading data: {str(e)}\")\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"   1. Make sure 'Data Treatment.ipynb' ran completely\")\n",
    "    print(f\"   2. Check if windowed data was saved successfully\")\n",
    "    print(f\"   3. Verify the processed_data directory exists\")\n",
    "    print(f\"   4. Check if pickle files are corrupted\")\n",
    "    print(f\"   5. Try using parquet format instead\")\n",
    "\n",
    "    # Show detailed error information\n",
    "    import traceback\n",
    "\n",
    "    print(f\"\\nDetailed error information:\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "    # Initialize empty variables for error case\n",
    "    normal_windows = []\n",
    "    normal_classes = []\n",
    "    anomaly_windows = []\n",
    "    anomaly_classes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229fa361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Autoencoder for Anomaly Detection\n",
      "=============================================\n",
      "‚úÖ Data available for processing\n",
      "Normal operation windows: 2000\n",
      "Anomaly windows: 1000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORT REQUIRED LIBRARIES AND MODULES\n",
    "# ============================================================\n",
    "\n",
    "print(\"LSTM Autoencoder for Anomaly Detection\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import our custom modules\n",
    "from src.autoencoder_models import StableLSTMAutoencoder\n",
    "from src.unsupervised_preprocessing import UnsupervisedDataPreprocessor\n",
    "from src.anomaly_detection import AnomalyDetector, visualize_latent_space\n",
    "\n",
    "# Set TensorFlow to avoid NaN issues\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "# Check if we have loaded data from previous cell\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "    and \"anomaly_windows\" in locals()\n",
    "    and anomaly_windows is not None\n",
    "    and len(anomaly_windows) > 0\n",
    "):\n",
    "\n",
    "    print(\"‚úÖ Data available for processing\")\n",
    "    print(f\"Normal operation windows: {len(normal_windows)}\")\n",
    "    print(f\"Anomaly windows: {len(anomaly_windows)}\")\n",
    "    data_ready = True\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå No data available. Please run the previous cell first to load normal and anomaly data.\"\n",
    "    )\n",
    "    data_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647600ad",
   "metadata": {},
   "source": [
    "## üß© Modular LSTM Autoencoder Implementation\n",
    "\n",
    "This section implements a **stable LSTM autoencoder** for anomaly detection using a **modular approach**. The code has been organized into reusable components in the `src/` directory:\n",
    "\n",
    "### üì¶ New Modules Created:\n",
    "\n",
    "1. **`autoencoder_models.py`** - Contains the `StableLSTMAutoencoder` class with:\n",
    "   - Stable architecture with gradient clipping\n",
    "   - Conservative hyperparameters for numerical stability\n",
    "   - Built-in training and prediction methods\n",
    "\n",
    "2. **`unsupervised_preprocessing.py`** - Contains the `UnsupervisedDataPreprocessor` class with:\n",
    "   - Smart data sampling for training efficiency\n",
    "   - Robust data validation and conversion\n",
    "   - Numerical stability enhancements\n",
    "\n",
    "3. **`anomaly_detection.py`** - Contains the `AnomalyDetector` class with:\n",
    "   - Reconstruction error computation\n",
    "   - Threshold determination methods\n",
    "   - Performance evaluation and visualization\n",
    "\n",
    "### üéØ Benefits of This Approach:\n",
    "\n",
    "- **Modularity**: Each component has a single responsibility\n",
    "- **Reusability**: Classes can be used in other projects\n",
    "- **Maintainability**: Easier to debug and modify individual components\n",
    "- **Stability**: Enhanced numerical safeguards and error handling\n",
    "- **Clarity**: Notebook cells are focused and easier to understand\n",
    "\n",
    "### üöÄ Usage Pattern:\n",
    "\n",
    "The following cells demonstrate the complete pipeline from data preprocessing through model training to anomaly detection evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c78e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Pipeline\n",
      "===================================\n",
      "üîß Complete Data Preparation Pipeline\n",
      "==================================================\n",
      "‚ö° Smart Data Sampling for Training Efficiency\n",
      "==================================================\n",
      "üéØ Training optimization settings:\n",
      "   ‚Ä¢ Max normal samples for training: 1000\n",
      "   ‚Ä¢ Max anomaly samples for testing: 300\n",
      "üìä Sampling 1000 normal windows from 2000 available...\n",
      "üìä Sampling 300 anomaly windows from 1000 available...\n",
      "üìä Converting normal windows to arrays... ‚úÖ (1000 valid processed)\n",
      "üìä Converting anomaly windows to arrays... ‚úÖ (300 valid processed)\n",
      "üîç Validating array shapes and data quality...\n",
      "Expected shape: (300, 3)\n",
      "‚úÖ Valid arrays: 1000 normal, 300 anomaly\n",
      "üìä Data quality checks:\n",
      "   ‚Ä¢ Normal data range: [0.000, 1.000]\n",
      "   ‚Ä¢ Anomaly data range: [0.000, 1.000]\n",
      "   ‚Ä¢ Normal data finite: True\n",
      "   ‚Ä¢ Anomaly data finite: True\n",
      "üìä Additional Data Normalization for Stability\n",
      "=============================================\n",
      "üìè Enhanced data characteristics:\n",
      "   ‚Ä¢ Normal data range: [0.001, 0.999]\n",
      "   ‚Ä¢ Anomaly data range: [0.001, 0.999]\n",
      "   ‚Ä¢ Data clipped to avoid extreme values\n",
      "   ‚Ä¢ Float32 precision for stability\n",
      "\n",
      "üìê Final Data Shapes:\n",
      "   ‚Ä¢ Normal data: (1000, 300, 3)\n",
      "   ‚Ä¢ Anomaly data: (300, 300, 3)\n",
      "   ‚Ä¢ Time steps per window: 300\n",
      "   ‚Ä¢ Features per time step: 3 (class column removed)\n",
      "\n",
      "‚úÖ Data preprocessing completed successfully!\n",
      "Final Configuration:\n",
      "   ‚Ä¢ Time steps per window: 300\n",
      "   ‚Ä¢ Features per time step: 3 (class column removed)\n",
      "   ‚Ä¢ Normal training samples: 1000\n",
      "   ‚Ä¢ Anomaly test samples: 300\n",
      "üìä Converting normal windows to arrays... ‚úÖ (1000 valid processed)\n",
      "üìä Converting anomaly windows to arrays... ‚úÖ (300 valid processed)\n",
      "üîç Validating array shapes and data quality...\n",
      "Expected shape: (300, 3)\n",
      "‚úÖ Valid arrays: 1000 normal, 300 anomaly\n",
      "üìä Data quality checks:\n",
      "   ‚Ä¢ Normal data range: [0.000, 1.000]\n",
      "   ‚Ä¢ Anomaly data range: [0.000, 1.000]\n",
      "   ‚Ä¢ Normal data finite: True\n",
      "   ‚Ä¢ Anomaly data finite: True\n",
      "üìä Additional Data Normalization for Stability\n",
      "=============================================\n",
      "üìè Enhanced data characteristics:\n",
      "   ‚Ä¢ Normal data range: [0.001, 0.999]\n",
      "   ‚Ä¢ Anomaly data range: [0.001, 0.999]\n",
      "   ‚Ä¢ Data clipped to avoid extreme values\n",
      "   ‚Ä¢ Float32 precision for stability\n",
      "\n",
      "üìê Final Data Shapes:\n",
      "   ‚Ä¢ Normal data: (1000, 300, 3)\n",
      "   ‚Ä¢ Anomaly data: (300, 300, 3)\n",
      "   ‚Ä¢ Time steps per window: 300\n",
      "   ‚Ä¢ Features per time step: 3 (class column removed)\n",
      "\n",
      "‚úÖ Data preprocessing completed successfully!\n",
      "Final Configuration:\n",
      "   ‚Ä¢ Time steps per window: 300\n",
      "   ‚Ä¢ Features per time step: 3 (class column removed)\n",
      "   ‚Ä¢ Normal training samples: 1000\n",
      "   ‚Ä¢ Anomaly test samples: 300\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATA PREPROCESSING AND SAMPLING\n",
    "# ============================================================\n",
    "\n",
    "if data_ready:\n",
    "    print(\"Data Preprocessing Pipeline\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Initialize data preprocessor with conservative settings for stability\n",
    "    preprocessor = UnsupervisedDataPreprocessor(\n",
    "        max_training_samples=1000,  # Reduced for stability\n",
    "        max_anomaly_samples=300,  # Reduced for stability\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Run the complete preprocessing pipeline\n",
    "    normal_scaled, anomaly_scaled, data_info = preprocessor.prepare_full_pipeline(\n",
    "        normal_windows, anomaly_windows\n",
    "    )\n",
    "\n",
    "    print(f\"\\n‚úÖ Data preprocessing completed successfully!\")\n",
    "    print(f\"Final Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Time steps per window: {data_info['time_steps']}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Features per time step: {data_info['n_features']} (class column removed)\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Normal training samples: {data_info['n_normal_samples']}\")\n",
    "    print(f\"   ‚Ä¢ Anomaly test samples: {data_info['n_anomaly_samples']}\")\n",
    "\n",
    "    preprocessing_complete = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed with preprocessing - no data available\")\n",
    "    preprocessing_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee4b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building and Training LSTM Autoencoder\n",
      "=============================================\n",
      "üèóÔ∏è Building Stable LSTM Autoencoder:\n",
      "   ‚Ä¢ Input shape: (300, 3)\n",
      "   ‚Ä¢ Encoder LSTM units: 32\n",
      "   ‚Ä¢ Latent dimension: 16\n",
      "   ‚Ä¢ Decoder LSTM units: 32\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "‚úÖ Stable LSTM Autoencoder model created\n",
      "   ‚Ä¢ Total parameters: 11,507\n",
      "   ‚Ä¢ Gradient clipping enabled (clipnorm=1.0)\n",
      "   ‚Ä¢ Conservative learning rate (0.0005)\n",
      "\n",
      "Training Data Split:\n",
      "   ‚Ä¢ Training samples: 800\n",
      "   ‚Ä¢ Validation samples: 200\n",
      "\n",
      "Starting Training...\n",
      "üöÇ Training LSTM Autoencoder:\n",
      "   ‚Ä¢ Training samples: 800\n",
      "   ‚Ä¢ Validation samples: 200\n",
      "   ‚Ä¢ Max epochs: 30\n",
      "   ‚Ä¢ Batch size: 32\n",
      "Epoch 1/30\n",
      "‚úÖ Stable LSTM Autoencoder model created\n",
      "   ‚Ä¢ Total parameters: 11,507\n",
      "   ‚Ä¢ Gradient clipping enabled (clipnorm=1.0)\n",
      "   ‚Ä¢ Conservative learning rate (0.0005)\n",
      "\n",
      "Training Data Split:\n",
      "   ‚Ä¢ Training samples: 800\n",
      "   ‚Ä¢ Validation samples: 200\n",
      "\n",
      "Starting Training...\n",
      "üöÇ Training LSTM Autoencoder:\n",
      "   ‚Ä¢ Training samples: 800\n",
      "   ‚Ä¢ Validation samples: 200\n",
      "   ‚Ä¢ Max epochs: 30\n",
      "   ‚Ä¢ Batch size: 32\n",
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 199ms/step - loss: 0.0992 - mae: 0.2734 - val_loss: 0.0641 - val_mae: 0.2200 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 199ms/step - loss: 0.0992 - mae: 0.2734 - val_loss: 0.0641 - val_mae: 0.2200 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 0.0564 - mae: 0.1968 - val_loss: 0.0404 - val_mae: 0.1585 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 0.0564 - mae: 0.1968 - val_loss: 0.0404 - val_mae: 0.1585 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0430 - mae: 0.1636 - val_loss: 0.0262 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0430 - mae: 0.1636 - val_loss: 0.0262 - val_mae: 0.1273 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - loss: 0.0378 - mae: 0.1508 - val_loss: 0.0208 - val_mae: 0.1114 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - loss: 0.0378 - mae: 0.1508 - val_loss: 0.0208 - val_mae: 0.1114 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 0.0320 - mae: 0.1358 - val_loss: 0.0150 - val_mae: 0.0910 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 0.0320 - mae: 0.1358 - val_loss: 0.0150 - val_mae: 0.0910 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0293 - mae: 0.1262 - val_loss: 0.0121 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0293 - mae: 0.1262 - val_loss: 0.0121 - val_mae: 0.0790 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0274 - mae: 0.1211 - val_loss: 0.0114 - val_mae: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0274 - mae: 0.1211 - val_loss: 0.0114 - val_mae: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0258 - mae: 0.1167 - val_loss: 0.0106 - val_mae: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0258 - mae: 0.1167 - val_loss: 0.0106 - val_mae: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 0.0234 - mae: 0.1101 - val_loss: 0.0103 - val_mae: 0.0695 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 0.0234 - mae: 0.1101 - val_loss: 0.0103 - val_mae: 0.0695 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0248 - mae: 0.1110 - val_loss: 0.0099 - val_mae: 0.0675 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0248 - mae: 0.1110 - val_loss: 0.0099 - val_mae: 0.0675 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0241 - mae: 0.1090 - val_loss: 0.0105 - val_mae: 0.0724 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0241 - mae: 0.1090 - val_loss: 0.0105 - val_mae: 0.0724 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0223 - mae: 0.1045 - val_loss: 0.0097 - val_mae: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0223 - mae: 0.1045 - val_loss: 0.0097 - val_mae: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0214 - mae: 0.1017 - val_loss: 0.0098 - val_mae: 0.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 0.0214 - mae: 0.1017 - val_loss: 0.0098 - val_mae: 0.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0228 - mae: 0.1024 - val_loss: 0.0099 - val_mae: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 0.0228 - mae: 0.1024 - val_loss: 0.0099 - val_mae: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0220 - mae: 0.1005 - val_loss: 0.0093 - val_mae: 0.0626 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0220 - mae: 0.1005 - val_loss: 0.0093 - val_mae: 0.0626 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0217 - mae: 0.0998 - val_loss: 0.0099 - val_mae: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0217 - mae: 0.0998 - val_loss: 0.0099 - val_mae: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0204 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0204 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.0203 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 0.0203 - mae: 0.0967 - val_loss: 0.0094 - val_mae: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0201 - mae: 0.0948 - val_loss: 0.0087 - val_mae: 0.0603 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0201 - mae: 0.0948 - val_loss: 0.0087 - val_mae: 0.0603 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0192 - mae: 0.0935 - val_loss: 0.0096 - val_mae: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 0.0192 - mae: 0.0935 - val_loss: 0.0096 - val_mae: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 21/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m training_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 33\u001b[0m training_success \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_normal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conservative for stability\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Conservative for stability\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m training_start\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\Documents\\GitHub\\3W\\resources\\introduction_to_ml_applied_to_mts\\src\\autoencoder_models.py:160\u001b[0m, in \u001b[0;36mStableLSTMAutoencoder.train\u001b[1;34m(self, train_data, val_data, epochs, batch_size, verbose)\u001b[0m\n\u001b[0;32m    155\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[0;32m    156\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Train autoencoder\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Autoencoder: input = target\u001b[39;49;00m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Check if training was successful\u001b[39;00m\n\u001b[0;32m    172\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\.conda\\envs\\3W-ml-mts\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD AND TRAIN LSTM AUTOENCODER\n",
    "# ============================================================\n",
    "\n",
    "if preprocessing_complete:\n",
    "    print(\"Building and Training LSTM Autoencoder\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    # Initialize the autoencoder with conservative parameters for stability\n",
    "    autoencoder = StableLSTMAutoencoder(\n",
    "        time_steps=data_info[\"time_steps\"],\n",
    "        n_features=data_info[\"n_features\"],\n",
    "        latent_dim=16,  # Conservative for stability\n",
    "        lstm_units=32,  # Conservative for stability\n",
    "    )\n",
    "\n",
    "    # Build the model\n",
    "    model = autoencoder.build_model()\n",
    "\n",
    "    # Split normal data into train/validation (80/20 split)\n",
    "    split_idx = int(0.8 * len(normal_scaled))\n",
    "    train_normal = normal_scaled[:split_idx]\n",
    "    val_normal = normal_scaled[split_idx:]\n",
    "\n",
    "    print(f\"\\nTraining Data Split:\")\n",
    "    print(f\"   ‚Ä¢ Training samples: {len(train_normal)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {len(val_normal)}\")\n",
    "\n",
    "    # Train the autoencoder\n",
    "    print(f\"\\nStarting Training...\")\n",
    "    training_start = time.time()\n",
    "\n",
    "    training_success = autoencoder.train(\n",
    "        train_data=train_normal,\n",
    "        val_data=val_normal,\n",
    "        epochs=30,  # Conservative for stability\n",
    "        batch_size=32,  # Conservative for stability\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    if training_success:\n",
    "        print(\"‚úÖ Model trained successfully\")\n",
    "        model_ready = True\n",
    "    else:\n",
    "        print(\"‚ùå Training failed - check for numerical stability issues\")\n",
    "        model_ready = False\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot build model - preprocessing not completed\")\n",
    "    model_ready = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "if model_ready and autoencoder.history is not None:\n",
    "    print(\"üìä Training History Visualization\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    history = autoencoder.history.history\n",
    "\n",
    "    # Create training plots\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history[\"loss\"], label=\"Training Loss\", linewidth=2, color=\"blue\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", linewidth=2, color=\"orange\")\n",
    "    plt.title(\"Model Loss During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # MAE plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history[\"mae\"], label=\"Training MAE\", linewidth=2, color=\"blue\")\n",
    "    plt.plot(history[\"val_mae\"], label=\"Validation MAE\", linewidth=2, color=\"orange\")\n",
    "    plt.title(\"Model MAE During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Learning rate plot (if available)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if \"lr\" in history:\n",
    "        plt.plot(history[\"lr\"], label=\"Learning Rate\", linewidth=2, color=\"green\")\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning Rate\")\n",
    "        plt.yscale(\"log\")\n",
    "    else:\n",
    "        plt.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Learning Rate\\nHistory\\nNot Available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\"),\n",
    "        )\n",
    "        plt.title(\"Learning Rate Schedule\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print training summary\n",
    "    final_train_loss = history[\"loss\"][-1]\n",
    "    final_val_loss = history[\"val_loss\"][-1]\n",
    "    epochs_trained = len(history[\"loss\"])\n",
    "\n",
    "    print(f\"\\nüìã Training Summary:\")\n",
    "    print(f\"   ‚Ä¢ Epochs trained: {epochs_trained}\")\n",
    "    print(f\"   ‚Ä¢ Final training loss: {final_train_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Final validation loss: {final_val_loss:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    if final_val_loss < final_train_loss * 1.5:\n",
    "        print(\"   ‚úÖ No significant overfitting detected\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Possible overfitting - validation loss higher than expected\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No training history available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ANOMALY DETECTION AND EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "if model_ready:\n",
    "    print(\"üîç Anomaly Detection and Evaluation\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    # Initialize anomaly detector\n",
    "    detector = AnomalyDetector()\n",
    "\n",
    "    # Compute reconstruction errors\n",
    "    normal_errors, anomaly_errors = detector.compute_reconstruction_errors(\n",
    "        autoencoder=autoencoder, normal_data=normal_scaled, anomaly_data=anomaly_scaled\n",
    "    )\n",
    "\n",
    "    # Determine threshold using 95th percentile of normal errors\n",
    "    threshold = detector.determine_threshold(method=\"percentile\", percentile=95)\n",
    "\n",
    "    # Evaluate detection performance\n",
    "    metrics = detector.evaluate_detection()\n",
    "\n",
    "    print(f\"\\nüéØ Anomaly Detection Results:\")\n",
    "    print(f\"   ‚Ä¢ Threshold: {threshold:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Normal accuracy: {metrics['normal_accuracy']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Anomaly accuracy: {metrics['anomaly_accuracy']:.3f}\")\n",
    "    print(f\"   ‚Ä¢ Overall accuracy: {metrics['overall_accuracy']:.3f}\")\n",
    "\n",
    "    detection_complete = True\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform anomaly detection - model not ready\")\n",
    "    detection_complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RESULTS VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\"üìä Comprehensive Results Visualization\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Plot reconstruction error distributions\n",
    "    print(\"üìà Plotting reconstruction error distributions...\")\n",
    "    detector.plot_error_distributions(figsize=(15, 10))\n",
    "\n",
    "    # Plot ROC curve\n",
    "    print(\"üìà Plotting ROC curve...\")\n",
    "    roc_auc = detector.plot_roc_curve()\n",
    "\n",
    "    print(f\"\\nüìä Visualization Summary:\")\n",
    "    print(f\"   ‚úÖ Error distribution plots generated\")\n",
    "    print(f\"   ‚úÖ ROC curve generated (AUC: {roc_auc:.3f})\")\n",
    "    print(f\"   ‚úÖ Comprehensive analysis completed\")\n",
    "\n",
    "    if roc_auc > 0.8:\n",
    "        print(f\"   üéâ Excellent anomaly detection performance!\")\n",
    "    elif roc_auc > 0.7:\n",
    "        print(f\"   üëç Good anomaly detection performance\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Consider tuning model parameters for better performance\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot generate visualizations - detection not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f25ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LATENT SPACE VISUALIZATION (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\"üé® Advanced Analysis: Latent Space Visualization\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Visualize latent space with t-SNE\n",
    "        visualize_latent_space(\n",
    "            autoencoder=autoencoder,\n",
    "            normal_data=normal_scaled,\n",
    "            anomaly_data=anomaly_scaled,\n",
    "            n_samples=500,  # Sample for faster computation\n",
    "        )\n",
    "\n",
    "        print(f\"\\n‚úÖ Latent space visualization completed\")\n",
    "        print(f\"   ‚Ä¢ This visualization shows how the autoencoder learns to\")\n",
    "        print(f\"     separate normal and anomalous patterns in its internal\")\n",
    "        print(f\"     latent representation\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Latent space visualization failed: {str(e)}\")\n",
    "        print(f\"   ‚Ä¢ This is optional and doesn't affect the main analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot visualize latent space - detection not completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d414dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reconstruction Error Analysis and Anomaly Threshold\n",
      "============================================================\n",
      "\n",
      "üìä Computing Reconstruction Errors\n",
      "========================================\n",
      "üîµ Computing reconstruction errors for normal data... ‚úÖ\n",
      "üî¥ Computing reconstruction errors for anomaly data... ‚úÖ\n",
      "üî¥ Computing reconstruction errors for anomaly data... ‚úÖ\n",
      "\n",
      "üìà Statistical Threshold Determination\n",
      "==========================================\n",
      "üìä Normal Data Reconstruction Error Statistics:\n",
      "   ‚Ä¢ Mean error: 0.014571\n",
      "   ‚Ä¢ Standard deviation: 0.016475\n",
      "   ‚Ä¢ Min error: 0.000585\n",
      "   ‚Ä¢ Max error: 0.194340\n",
      "\n",
      "üéØ Anomaly Detection Threshold:\n",
      "   ‚Ä¢ Threshold (Œº + 3œÉ): 0.047522\n",
      "   ‚Ä¢ Confidence level: 99.7%\n",
      "   ‚Ä¢ Normal samples above threshold: 49 / 1000\n",
      "   ‚Ä¢ Normal false positive rate: 4.90%\n",
      "\n",
      "üî¥ Anomaly Detection Performance:\n",
      "   ‚Ä¢ Anomaly samples above threshold: 125 / 300\n",
      "   ‚Ä¢ Anomaly detection rate: 41.67%\n",
      "\n",
      "üìä Anomaly Detection by Class:\n",
      "‚úÖ\n",
      "\n",
      "üìà Statistical Threshold Determination\n",
      "==========================================\n",
      "üìä Normal Data Reconstruction Error Statistics:\n",
      "   ‚Ä¢ Mean error: 0.014571\n",
      "   ‚Ä¢ Standard deviation: 0.016475\n",
      "   ‚Ä¢ Min error: 0.000585\n",
      "   ‚Ä¢ Max error: 0.194340\n",
      "\n",
      "üéØ Anomaly Detection Threshold:\n",
      "   ‚Ä¢ Threshold (Œº + 3œÉ): 0.047522\n",
      "   ‚Ä¢ Confidence level: 99.7%\n",
      "   ‚Ä¢ Normal samples above threshold: 49 / 1000\n",
      "   ‚Ä¢ Normal false positive rate: 4.90%\n",
      "\n",
      "üî¥ Anomaly Detection Performance:\n",
      "   ‚Ä¢ Anomaly samples above threshold: 125 / 300\n",
      "   ‚Ä¢ Anomaly detection rate: 41.67%\n",
      "\n",
      "üìä Anomaly Detection by Class:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 300 but size of corresponding boolean axis is 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m unique_anomaly_classes:\n\u001b[32m     68\u001b[39m     cls_mask = np.array(anomaly_classes) == \u001b[38;5;28mcls\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     cls_errors = \u001b[43manomaly_errors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcls_mask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     70\u001b[39m     cls_detected = np.sum(cls_errors > threshold)\n\u001b[32m     71\u001b[39m     cls_total = \u001b[38;5;28mlen\u001b[39m(cls_errors)\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along axis 0; size of axis is 300 but size of corresponding boolean axis is 1000"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SUMMARY AND NEXT STEPS\n",
    "# ============================================================\n",
    "\n",
    "if detection_complete:\n",
    "    print(\" Unsupervised Learning Summary\")\n",
    "    print(\"=\" * 35)\n",
    "\n",
    "    print(f\"‚úÖ Complete LSTM Autoencoder Pipeline Executed:\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Data preprocessing: {data_info['n_normal_samples']} normal, {data_info['n_anomaly_samples']} anomaly samples\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Model architecture: {autoencoder.lstm_units} LSTM units, {autoencoder.latent_dim} latent dimensions\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Training: {len(autoencoder.history.history['loss'])} epochs, final loss: {autoencoder.history.history['loss'][-1]:.6f}\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Anomaly detection: {metrics['overall_accuracy']:.3f} overall accuracy\")\n",
    "    print(f\"   ‚Ä¢ ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "    print(f\"\\nüéì Key Learning Outcomes:\")\n",
    "    print(f\"   ‚Ä¢ LSTM autoencoders can learn normal operation patterns\")\n",
    "    print(f\"   ‚Ä¢ Reconstruction errors effectively identify anomalies\")\n",
    "    print(f\"   ‚Ä¢ Threshold selection critically impacts detection performance\")\n",
    "    print(f\"   ‚Ä¢ Latent space visualization reveals learned representations\")\n",
    "\n",
    "    print(f\"\\nüöÄ Next Steps for Advanced Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Experiment with different autoencoder architectures\")\n",
    "    print(f\"   ‚Ä¢ Try variational autoencoders (VAEs) for uncertainty quantification\")\n",
    "    print(f\"   ‚Ä¢ Implement online anomaly detection for real-time monitoring\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Compare with other unsupervised methods (Isolation Forest, One-Class SVM)\"\n",
    "    )\n",
    "\n",
    "    # Store final results for potential further analysis\n",
    "    final_results = {\n",
    "        \"autoencoder\": autoencoder,\n",
    "        \"detector\": detector,\n",
    "        \"metrics\": metrics,\n",
    "        \"data_info\": data_info,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"normal_scaled\": normal_scaled,\n",
    "        \"anomaly_scaled\": anomaly_scaled,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nüíæ Results stored in 'final_results' for further analysis\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Analysis incomplete - please run all previous cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3426af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADVANCED LATENT SPACE ANALYSIS (OPTIONAL)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üåå Advanced Latent Space Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "if \"final_results\" in locals() and final_results is not None:\n",
    "\n",
    "    # Extract components from results\n",
    "    autoencoder_model = final_results[\"autoencoder\"]\n",
    "    normal_data = final_results[\"normal_scaled\"]\n",
    "    anomaly_data = final_results[\"anomaly_scaled\"]\n",
    "\n",
    "    print(\"üß† Extracting and Analyzing Latent Representations\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Create encoder model to extract latent representations\n",
    "        encoder_input = autoencoder_model.model.input\n",
    "        encoder_output = autoencoder_model.model.get_layer(\"latent\").output\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "\n",
    "        # Extract latent representations\n",
    "        print(\"üîµ Computing latent representations for normal data...\")\n",
    "        normal_latent = encoder.predict(\n",
    "            normal_data[:500], verbose=0\n",
    "        )  # Sample for efficiency\n",
    "\n",
    "        print(\"üî¥ Computing latent representations for anomaly data...\")\n",
    "        anomaly_latent = encoder.predict(\n",
    "            anomaly_data[:200], verbose=0\n",
    "        )  # Sample for efficiency\n",
    "\n",
    "        print(f\"üìä Latent space analysis:\")\n",
    "        print(f\"   ‚Ä¢ Normal latent shape: {normal_latent.shape}\")\n",
    "        print(f\"   ‚Ä¢ Anomaly latent shape: {anomaly_latent.shape}\")\n",
    "        print(f\"   ‚Ä¢ Latent dimension: {normal_latent.shape[1]}\")\n",
    "\n",
    "        # Combine for analysis\n",
    "        all_latent = np.vstack([normal_latent, anomaly_latent])\n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                np.zeros(len(normal_latent)),  # 0 for normal\n",
    "                np.ones(len(anomaly_latent)),  # 1 for anomaly\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Latent space statistics\n",
    "        normal_mean = np.mean(normal_latent, axis=0)\n",
    "        anomaly_mean = np.mean(anomaly_latent, axis=0)\n",
    "        latent_separation = np.linalg.norm(normal_mean - anomaly_mean)\n",
    "\n",
    "        print(f\"\\n Latent Space Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Normal latent mean magnitude: {np.linalg.norm(normal_mean):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Anomaly latent mean magnitude: {np.linalg.norm(anomaly_mean):.3f}\")\n",
    "        print(f\"   ‚Ä¢ Mean separation distance: {latent_separation:.3f}\")\n",
    "\n",
    "        # Simple 2D visualization if latent dimension allows\n",
    "        if normal_latent.shape[1] >= 2:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "\n",
    "            # Plot first two latent dimensions\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.scatter(\n",
    "                normal_latent[:, 0],\n",
    "                normal_latent[:, 1],\n",
    "                alpha=0.6,\n",
    "                label=\"Normal\",\n",
    "                color=\"blue\",\n",
    "                s=20,\n",
    "            )\n",
    "            plt.scatter(\n",
    "                anomaly_latent[:, 0],\n",
    "                anomaly_latent[:, 1],\n",
    "                alpha=0.6,\n",
    "                label=\"Anomaly\",\n",
    "                color=\"red\",\n",
    "                s=20,\n",
    "            )\n",
    "            plt.xlabel(\"Latent Dimension 1\")\n",
    "            plt.ylabel(\"Latent Dimension 2\")\n",
    "            plt.title(\"Latent Space (First 2 Dimensions)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            # Plot latent dimension distributions\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist(\n",
    "                normal_latent[:, 0],\n",
    "                bins=30,\n",
    "                alpha=0.7,\n",
    "                label=\"Normal (Dim 1)\",\n",
    "                color=\"blue\",\n",
    "                density=True,\n",
    "            )\n",
    "            plt.hist(\n",
    "                anomaly_latent[:, 0],\n",
    "                bins=30,\n",
    "                alpha=0.7,\n",
    "                label=\"Anomaly (Dim 1)\",\n",
    "                color=\"red\",\n",
    "                density=True,\n",
    "            )\n",
    "            plt.xlabel(\"Latent Value\")\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.title(\"Latent Dimension 1 Distribution\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        print(f\"\\n‚úÖ Advanced latent space analysis completed\")\n",
    "        print(f\"   ‚Ä¢ The latent space shows how the autoencoder compresses\")\n",
    "        print(f\"     time series data into a lower-dimensional representation\")\n",
    "        print(f\"   ‚Ä¢ Separation in latent space indicates learned differences\")\n",
    "        print(f\"     between normal and anomalous patterns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Advanced latent analysis failed: {str(e)}\")\n",
    "        print(f\"   ‚Ä¢ This is optional and doesn't affect the main results\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results available for latent space analysis\")\n",
    "    print(\"   ‚Ä¢ Please run all previous cells to generate results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c07b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Individual Algorithm Training Example\n",
      "==================================================\n",
      "üìä Comprehensive classification already completed above!\n",
      "üîç Here's how to access individual algorithm results:\n",
      "\n",
      "üå≥ Tree-Based Models Performance:\n",
      "----------------------------------------\n",
      "Decision Tree:\n",
      "   ‚Ä¢ Training Accuracy: 0.657\n",
      "   ‚Ä¢ Test Accuracy: 0.389\n",
      "   ‚Ä¢ Training Time: 3.906s\n",
      "\n",
      "Random Forest:\n",
      "   ‚Ä¢ Training Accuracy: 0.866\n",
      "   ‚Ä¢ Test Accuracy: 0.531\n",
      "   ‚Ä¢ Training Time: 2.421s\n",
      "   ‚Ä¢ Top 5 Most Important Features:\n",
      "     1. Feature 593: 0.0087\n",
      "     2. Feature 577: 0.0072\n",
      "     3. Feature 581: 0.0065\n",
      "     4. Feature 595: 0.0063\n",
      "     5. Feature 569: 0.0062\n",
      "\n",
      "üí° To train individual algorithms separately:\n",
      "   1. Use supervised_classifier.prepare_data() to get X_train, y_train, X_test, y_test\n",
      "   2. Call supervised_classifier.train_decision_trees(X_train, y_train, X_test, y_test)\n",
      "   3. Or use supervised_classifier.train_svm() or train_neural_networks()\n",
      "\n",
      "üîß Example: Training only Decision Trees individually\n",
      "(This would be useful if you only want specific algorithms)\n",
      "‚úÖ Data preparation, class balancing, and augmentation already handled by module\n",
      "‚úÖ All models already trained - see comprehensive results above\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LSTM AUTOENCODER FOR NOVELTY DETECTION\n",
    "# ============================================================\n",
    "\n",
    "print(\"ü§ñ LSTM Autoencoder Novelty Detection Implementation\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Check if we have loaded data from previous cell\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "    and \"anomaly_windows\" in locals()\n",
    "    and anomaly_windows is not None\n",
    "    and len(anomaly_windows) > 0\n",
    "):\n",
    "\n",
    "    # Import required libraries\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.layers import (\n",
    "        Input,\n",
    "        LSTM,\n",
    "        Dense,\n",
    "        RepeatVector,\n",
    "        TimeDistributed,\n",
    "        Dropout,\n",
    "    )\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    import time\n",
    "\n",
    "    print(\"üìä Using optimized LSTM Autoencoder for novelty detection...\")\n",
    "    print(f\"üü¢ Normal operation windows for training: {len(normal_windows)}\")\n",
    "    print(f\"üî¥ Anomaly windows for testing: {len(anomaly_windows)}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # SMART DATA SAMPLING FOR FASTER TRAINING\n",
    "    # ============================================================\n",
    "    print(\"\\n‚ö° Smart Data Sampling for Training Efficiency\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Use subset for faster training (configurable)\n",
    "    MAX_TRAINING_SAMPLES = 1500  # Reduced for faster training\n",
    "    MAX_ANOMALY_SAMPLES = 500  # Reduced for faster evaluation\n",
    "\n",
    "    print(f\"üéØ Training optimization settings:\")\n",
    "    print(f\"   ‚Ä¢ Max normal samples for training: {MAX_TRAINING_SAMPLES}\")\n",
    "    print(f\"   ‚Ä¢ Max anomaly samples for testing: {MAX_ANOMALY_SAMPLES}\")\n",
    "    print(f\"   ‚Ä¢ This ensures reasonable training time with good performance\")\n",
    "\n",
    "    # Sample normal data for training\n",
    "    if len(normal_windows) > MAX_TRAINING_SAMPLES:\n",
    "        print(\n",
    "            f\"üìä Sampling {MAX_TRAINING_SAMPLES} normal windows from {len(normal_windows)} available...\"\n",
    "        )\n",
    "        # Use random sampling to get diverse examples\n",
    "        import random\n",
    "\n",
    "        sampled_indices = random.sample(\n",
    "            range(len(normal_windows)), MAX_TRAINING_SAMPLES\n",
    "        )\n",
    "        sampled_normal_windows = [normal_windows[i] for i in sampled_indices]\n",
    "    else:\n",
    "        print(f\"üìä Using all {len(normal_windows)} normal windows...\")\n",
    "        sampled_normal_windows = normal_windows\n",
    "\n",
    "    # Sample anomaly data for testing\n",
    "    if len(anomaly_windows) > MAX_ANOMALY_SAMPLES:\n",
    "        print(\n",
    "            f\"üìä Sampling {MAX_ANOMALY_SAMPLES} anomaly windows from {len(anomaly_windows)} available...\"\n",
    "        )\n",
    "        sampled_indices = random.sample(\n",
    "            range(len(anomaly_windows)), MAX_ANOMALY_SAMPLES\n",
    "        )\n",
    "        sampled_anomaly_windows = [anomaly_windows[i] for i in sampled_indices]\n",
    "    else:\n",
    "        print(f\"üìä Using all {len(anomaly_windows)} anomaly windows...\")\n",
    "        sampled_anomaly_windows = anomaly_windows\n",
    "\n",
    "    # ============================================================\n",
    "    # DATA PREPARATION WITH PROGRESS FEEDBACK\n",
    "    # ============================================================\n",
    "    print(\"\\nüîß Preparing Data for LSTM Autoencoder\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    print(\"üìä Converting normal windows to arrays...\", end=\" \")\n",
    "    start_conversion = time.time()\n",
    "\n",
    "    # Convert normal windows to numpy arrays for training\n",
    "    normal_arrays = []\n",
    "    for i, window in enumerate(sampled_normal_windows):\n",
    "        if i % 200 == 0 and i > 0:\n",
    "            print(\n",
    "                f\"\\rüìä Converting normal windows to arrays... {i}/{len(sampled_normal_windows)}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "        # Ensure consistent shape and convert to numpy\n",
    "        window_array = window.values if hasattr(window, \"values\") else window\n",
    "        normal_arrays.append(window_array)\n",
    "\n",
    "    print(\n",
    "        f\"\\rüìä Converting normal windows to arrays... ‚úÖ ({len(normal_arrays)} processed)\"\n",
    "    )\n",
    "\n",
    "    print(\"üìä Converting anomaly windows to arrays...\", end=\" \")\n",
    "    # Convert anomaly windows to numpy arrays for testing\n",
    "    anomaly_arrays = []\n",
    "    for i, window in enumerate(sampled_anomaly_windows):\n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print(\n",
    "                f\"\\rüìä Converting anomaly windows to arrays... {i}/{len(sampled_anomaly_windows)}\",\n",
    "                end=\"\",\n",
    "            )\n",
    "        # Ensure consistent shape and convert to numpy\n",
    "        window_array = window.values if hasattr(window, \"values\") else window\n",
    "        anomaly_arrays.append(window_array)\n",
    "\n",
    "    print(\n",
    "        f\"\\rüìä Converting anomaly windows to arrays... ‚úÖ ({len(anomaly_arrays)} processed)\"\n",
    "    )\n",
    "\n",
    "    normal_data = np.array(normal_arrays)\n",
    "    anomaly_data = np.array(anomaly_arrays)\n",
    "\n",
    "    conversion_time = time.time() - start_conversion\n",
    "    print(f\"‚ö° Data conversion completed in {conversion_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"\\nüìê Data shapes:\")\n",
    "    print(f\"   ‚Ä¢ Normal data: {normal_data.shape}\")\n",
    "    print(f\"   ‚Ä¢ Anomaly data: {anomaly_data.shape}\")\n",
    "\n",
    "    # Get dimensions\n",
    "    n_normal_samples, time_steps, n_features = normal_data.shape\n",
    "\n",
    "    print(f\"\\nüìã LSTM Autoencoder Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Time steps per window: {time_steps}\")\n",
    "    print(f\"   ‚Ä¢ Features per time step: {n_features}\")\n",
    "    print(f\"   ‚Ä¢ Normal training samples: {n_normal_samples}\")\n",
    "    print(f\"   ‚Ä¢ Anomaly test samples: {anomaly_data.shape[0]}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # DATA PREPARATION (ALREADY SCALED)\n",
    "    # ============================================================\n",
    "    print(\"\\nüìä Data Already Preprocessed\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    print(\"‚úÖ Using pre-scaled windowed data from data treatment process\")\n",
    "    # Data is already normalized from the windowing process - no additional scaling needed\n",
    "    normal_scaled = normal_data\n",
    "    anomaly_scaled = anomaly_data\n",
    "\n",
    "    print(f\"üìè Data characteristics:\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Normal data range: [{np.min(normal_scaled):.3f}, {np.max(normal_scaled):.3f}]\"\n",
    "    )\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Anomaly data range: [{np.min(anomaly_scaled):.3f}, {np.max(anomaly_scaled):.3f}]\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Data already optimized for neural network training\")\n",
    "\n",
    "    # ============================================================\n",
    "    # OPTIMIZED LSTM AUTOENCODER ARCHITECTURE\n",
    "    # ============================================================\n",
    "    print(\"\\nüß† Building Optimized LSTM Autoencoder Architecture\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Optimized hyperparameters for faster training\n",
    "    latent_dim = 32  # Reduced from 64 for faster training\n",
    "    lstm_units = 64  # Reduced from 128 for faster training\n",
    "\n",
    "    print(f\"üèóÔ∏è Optimized Architecture Configuration:\")\n",
    "    print(f\"   ‚Ä¢ Input shape: ({time_steps}, {n_features})\")\n",
    "    print(f\"   ‚Ä¢ Encoder LSTM units: {lstm_units} (reduced for speed)\")\n",
    "    print(f\"   ‚Ä¢ Latent dimension: {latent_dim} (reduced for speed)\")\n",
    "    print(f\"   ‚Ä¢ Decoder LSTM units: {lstm_units}\")\n",
    "    print(f\"   ‚Ä¢ Output shape: ({time_steps}, {n_features})\")\n",
    "    print(f\"   ‚Ä¢ Dropout added for regularization\")\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(time_steps, n_features), name=\"input\")\n",
    "\n",
    "    # Encoder with dropout for regularization\n",
    "    encoded = LSTM(lstm_units, activation=\"relu\", name=\"encoder_lstm\")(input_layer)\n",
    "    encoded = Dropout(0.2, name=\"encoder_dropout\")(encoded)\n",
    "\n",
    "    # Latent representation (bottleneck)\n",
    "    latent = Dense(latent_dim, activation=\"relu\", name=\"latent\")(encoded)\n",
    "\n",
    "    # Decoder\n",
    "    decoded = RepeatVector(time_steps, name=\"repeat_vector\")(latent)\n",
    "    decoded = LSTM(\n",
    "        lstm_units, activation=\"relu\", return_sequences=True, name=\"decoder_lstm\"\n",
    "    )(decoded)\n",
    "    decoded = Dropout(0.2, name=\"decoder_dropout\")(decoded)\n",
    "    decoded = TimeDistributed(Dense(n_features), name=\"output\")(decoded)\n",
    "\n",
    "    # Create autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded, name=\"optimized_lstm_autoencoder\")\n",
    "\n",
    "    # Compile model with optimized settings\n",
    "    autoencoder.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=0.002\n",
    "        ),  # Slightly higher learning rate for faster convergence\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"],\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Optimized LSTM Autoencoder model created\")\n",
    "\n",
    "    # Display model summary (compact)\n",
    "    print(f\"\\nüìã Model Summary:\")\n",
    "    print(f\"   ‚Ä¢ Total parameters: {autoencoder.count_params():,}\")\n",
    "    print(\n",
    "        f\"   ‚Ä¢ Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in autoencoder.trainable_weights]):,}\"\n",
    "    )\n",
    "    print(f\"   ‚Ä¢ Model architecture optimized for faster training\")\n",
    "\n",
    "    # ============================================================\n",
    "    # OPTIMIZED MODEL TRAINING WITH PROGRESS FEEDBACK\n",
    "    # ============================================================\n",
    "    print(\"\\nüöÇ Training Optimized LSTM Autoencoder on Normal Data\")\n",
    "    print(\"=\" * 55)\n",
    "\n",
    "    # Split normal data into train/validation (80/20 split)\n",
    "    split_idx = int(0.8 * len(normal_scaled))\n",
    "    train_normal = normal_scaled[:split_idx]\n",
    "    val_normal = normal_scaled[split_idx:]\n",
    "\n",
    "    print(f\"üìä Training split:\")\n",
    "    print(f\"   ‚Ä¢ Training samples: {len(train_normal)}\")\n",
    "    print(f\"   ‚Ä¢ Validation samples: {len(val_normal)}\")\n",
    "\n",
    "    # Optimized callbacks for faster training\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,  # Reduced patience for faster training\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=0.0001, verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"üöÇ Starting optimized training...\")\n",
    "    print(f\"   ‚Ä¢ Max epochs: 50 (reduced for speed)\")\n",
    "    print(f\"   ‚Ä¢ Batch size: 64 (increased for efficiency)\")\n",
    "    print(f\"   ‚Ä¢ Early stopping patience: 5 epochs\")\n",
    "    print(f\"   ‚Ä¢ Learning rate reduction on plateau\")\n",
    "\n",
    "    training_start = time.time()\n",
    "\n",
    "    # Train autoencoder with optimized parameters\n",
    "    history = autoencoder.fit(\n",
    "        train_normal,\n",
    "        train_normal,  # Autoencoder: input = target\n",
    "        validation_data=(val_normal, val_normal),\n",
    "        epochs=50,  # Reduced from 100 for faster training\n",
    "        batch_size=64,  # Increased from 32 for faster training\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1,  # Show progress during training\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"   ‚Ä¢ Epochs trained: {len(history.history['loss'])}\")\n",
    "    print(f\"   ‚Ä¢ Final training loss: {history.history['loss'][-1]:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Final validation loss: {history.history['val_loss'][-1]:.6f}\")\n",
    "\n",
    "    # Quick training visualization\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\", linewidth=2)\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
    "    plt.title(\"Model Loss During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"mae\"], label=\"Training MAE\", linewidth=2)\n",
    "    plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\", linewidth=2)\n",
    "    plt.title(\"Model MAE During Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Absolute Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nüìä Training Summary:\")\n",
    "    print(f\"   ‚úÖ Model successfully trained on normal operation data\")\n",
    "    print(f\"   ‚úÖ Training time: {training_time:.2f} seconds\")\n",
    "    print(f\"   ‚úÖ Architecture optimized for speed and performance\")\n",
    "    print(f\"   ‚úÖ Ready for anomaly detection evaluation\")\n",
    "\n",
    "    print(f\"\\n‚ö° Performance Optimizations Applied:\")\n",
    "    print(f\"   ‚Ä¢ Reduced model complexity (64 LSTM units vs 128)\")\n",
    "    print(f\"   ‚Ä¢ Smaller latent dimension (32 vs 64)\")\n",
    "    print(f\"   ‚Ä¢ Smart data sampling for training efficiency\")\n",
    "    print(f\"   ‚Ä¢ Increased batch size for faster training\")\n",
    "    print(f\"   ‚Ä¢ Reduced epochs with early stopping\")\n",
    "    print(f\"   ‚Ä¢ Learning rate scheduling for optimal convergence\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"‚ùå No data available. Please run the previous cell first to load normal and anomaly data.\"\n",
    "    )\n",
    "    autoencoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe37606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK AUTOENCODER TEST (OPTIONAL - FOR IMMEDIATE FEEDBACK)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üî¨ Quick Autoencoder Test for Immediate Feedback\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# This cell provides immediate feedback to ensure everything works\n",
    "# Run this first if you want to test the setup before full training\n",
    "\n",
    "if (\n",
    "    \"normal_windows\" in locals()\n",
    "    and normal_windows is not None\n",
    "    and len(normal_windows) > 0\n",
    "):\n",
    "\n",
    "    # Quick test with minimal data\n",
    "    print(\"üöÄ Testing with minimal dataset for immediate feedback...\")\n",
    "\n",
    "    # Use only a tiny subset for testing\n",
    "    test_normal = normal_windows[:10]  # Just 10 samples for testing\n",
    "    test_anomaly = anomaly_windows[:5] if len(anomaly_windows) >= 5 else anomaly_windows\n",
    "\n",
    "    print(f\"üß™ Test data:\")\n",
    "    print(f\"   ‚Ä¢ Normal samples: {len(test_normal)}\")\n",
    "    print(f\"   ‚Ä¢ Anomaly samples: {len(test_anomaly)}\")\n",
    "\n",
    "    # Quick conversion test with class column removal\n",
    "    print(\"üìä Testing data conversion (removing class column)...\", end=\" \")\n",
    "    try:\n",
    "        test_normal_arrays = []\n",
    "        for window in test_normal:\n",
    "            # Get the DataFrame values and remove the class column\n",
    "            if hasattr(window, \"values\"):\n",
    "                window_data = window.copy()\n",
    "                # Remove 'class' column if it exists\n",
    "                if \"class\" in window_data.columns:\n",
    "                    window_data = window_data.drop(\"class\", axis=1)\n",
    "                    print(f\"\\n   üìã Removed 'class' column from DataFrame\")\n",
    "                window_array = window_data.values\n",
    "            else:\n",
    "                # If it's already an array, assume last column is class and remove it\n",
    "                if len(window.shape) == 2 and window.shape[1] > 1:\n",
    "                    window_array = window[:, :-1]  # Remove last column (class)\n",
    "                    print(f\"\\n   üìã Removed last column (assumed class) from array\")\n",
    "                else:\n",
    "                    window_array = window\n",
    "\n",
    "            test_normal_arrays.append(window_array)\n",
    "\n",
    "        test_normal_data = np.array(test_normal_arrays)\n",
    "        print(\"‚úÖ\")\n",
    "        print(f\"   ‚Ä¢ Test normal data shape: {test_normal_data.shape}\")\n",
    "\n",
    "        # Extract dimensions\n",
    "        test_samples, test_time_steps, test_features = test_normal_data.shape\n",
    "        print(f\"   ‚Ä¢ Time steps: {test_time_steps}\")\n",
    "        print(f\"   ‚Ä¢ Features: {test_features} (after removing class column)\")\n",
    "\n",
    "        # Check data characteristics\n",
    "        print(\n",
    "            f\"   ‚Ä¢ Data range: [{np.min(test_normal_data):.3f}, {np.max(test_normal_data):.3f}]\"\n",
    "        )\n",
    "        print(f\"   ‚Ä¢ Data type: {test_normal_data.dtype}\")\n",
    "\n",
    "        # Verify the first window columns if it's a DataFrame\n",
    "        if hasattr(test_normal[0], \"columns\"):\n",
    "            print(f\"   ‚Ä¢ Original columns: {list(test_normal[0].columns)}\")\n",
    "            if \"class\" in test_normal[0].columns:\n",
    "                print(\n",
    "                    f\"   ‚Ä¢ Features after removing class: {list(test_normal[0].drop('class', axis=1).columns)}\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\n‚úÖ Quick test successful! Data is ready for LSTM autoencoder.\")\n",
    "        print(f\"‚úÖ Class column properly handled and removed from features.\")\n",
    "        print(f\"üí° You can now run the full training cell with confidence.\")\n",
    "        print(f\"‚ö° Estimated full training time: ~2-5 minutes with optimized settings\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in quick test: {str(e)}\")\n",
    "        print(f\"üí° Please check the data loading cell and try again\")\n",
    "\n",
    "        # Show more detailed error information\n",
    "        import traceback\n",
    "\n",
    "        print(f\"\\nüîç Detailed error:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No normal_windows data available.\")\n",
    "    print(\"üí° Please run the data loading cell first.\")\n",
    "\n",
    "print(f\"\\nüìã Next Steps:\")\n",
    "print(f\"   1. ‚úÖ Quick test completed - data is compatible\")\n",
    "print(f\"   2. ‚úÖ Class column handling verified\")\n",
    "print(f\"   3. üöÇ Run the full training cell below for complete autoencoder\")\n",
    "print(f\"   4. üîç Proceed to anomaly detection evaluation\")\n",
    "print(f\"   5. üìä Visualize results and performance metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b91df2",
   "metadata": {},
   "source": [
    "##  Introduction to Autoencoders for Novelty Detection (5 minutes)\n",
    "\n",
    "### What are Autoencoders?\n",
    "\n",
    "**Autoencoders** are neural networks designed to learn efficient data representations by training the network to copy its input to its output. They learn to compress and then reconstruct data.\n",
    "\n",
    "### Autoencoder Architecture:\n",
    "- **Input Layer**: Original data (e.g., sensor readings)\n",
    "- **Encoder**: Compresses input into lower-dimensional representation\n",
    "- **Latent Space**: Compressed representation (bottleneck)\n",
    "- **Decoder**: Reconstructs original data from compressed representation\n",
    "- **Output Layer**: Reconstructed data (should match input)\n",
    "\n",
    "### How Autoencoders Detect Novelty:\n",
    "\n",
    "1. **Training Phase**: Learn to reconstruct only normal data\n",
    "2. **Normal Data**: Low reconstruction error (model learned these patterns)\n",
    "3. **Anomalous Data**: High reconstruction error (model never saw these patterns)\n",
    "4. **Threshold**: Statistical boundary between normal and anomalous reconstruction errors\n",
    "\n",
    "### Why Autoencoders for Oil Well Data:\n",
    "\n",
    "#### Advantages:\n",
    "- **Unsupervised Learning**: Only need normal operation data\n",
    "- **Feature Learning**: Automatically discover important sensor patterns\n",
    "- **Dimensionality Reduction**: Handle high-dimensional sensor data efficiently\n",
    "- **Non-linear Patterns**: Capture complex relationships between sensors\n",
    "- **Reconstruction-Based**: Intuitive interpretation of anomaly scores\n",
    "\n",
    "#### LSTM Autoencoders Specifically:\n",
    "- **Temporal Modeling**: Handle time series sensor data naturally\n",
    "- **Sequential Dependencies**: Capture patterns across time steps\n",
    "- **Variable Sequences**: Adapt to different operational phases\n",
    "- **Memory Cells**: Remember long-term normal operation patterns\n",
    "\n",
    "### Novelty Detection Process:\n",
    "\n",
    "1. **Data Preparation**: Normalize and structure time series data\n",
    "2. **Model Training**: Train autoencoder on normal data only\n",
    "3. **Error Computation**: Calculate reconstruction errors for all data\n",
    "4. **Threshold Setting**: Use statistical methods (Œº + 3œÉ) on normal errors\n",
    "5. **Anomaly Detection**: Flag samples with errors above threshold\n",
    "6. **Validation**: Test on known fault data to evaluate performance\n",
    "\n",
    "### Industrial Applications:\n",
    "- **Predictive Maintenance**: Detect equipment degradation early\n",
    "- **Quality Control**: Identify production anomalies\n",
    "- **System Monitoring**: Continuous health assessment\n",
    "- **Safety Systems**: Early warning for critical failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
